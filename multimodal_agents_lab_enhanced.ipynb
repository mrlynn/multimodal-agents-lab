{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Agents Workshop with Progress Tracking\n",
    "\n",
    "This enhanced version of the multimodal agents workshop includes comprehensive progress tracking, validation, and interactive guidance using the `jupyter-lab-progress` library.\n",
    "\n",
    "**Workshop Overview:**\n",
    "- Build a multimodal AI agent that can analyze documents and images\n",
    "- Use MongoDB Atlas Vector Search for retrieval\n",
    "- Implement function calling with Gemini 2.0 Flash\n",
    "- Add memory and ReAct reasoning capabilities\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "- Process PDFs and extract images for multimodal search\n",
    "- Set up MongoDB Atlas vector search indexes\n",
    "- Build an AI agent with tool calling capabilities\n",
    "- Implement session-based memory for conversational agents\n",
    "- Create a ReAct (Reasoning + Acting) agent architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize progress tracking and lab utilities\nimport sys\nimport os\n\n# Force load from development source if available\ndev_path = \"/Users/michael.lynn/code/mongodb/developer-days/jupyter-utils/jupyter-lab-progress\"\nif os.path.exists(dev_path) and dev_path not in sys.path:\n    sys.path.insert(0, dev_path)\n\n# Remove any cached modules\nmodules_to_remove = [key for key in sys.modules.keys() if key.startswith('jupyter_lab_progress')]\nfor module in modules_to_remove:\n    del sys.modules[module]\n\ntry:\n    from jupyter_lab_progress import (\n        LabProgress, LabValidator, show_info, show_warning, \n        show_success, show_error, show_hint\n    )\n    show_success(\"Progress tracking libraries loaded successfully! üéâ\")\nexcept ImportError as e:\n    print(f\"Warning: Could not import progress tracking: {e}\")\n    print(\"Installing basic fallbacks...\")\n    def show_info(msg, title=None): print(f\"‚ÑπÔ∏è {title or 'Info'}: {msg}\")\n    def show_warning(msg, title=None): print(f\"‚ö†Ô∏è {title or 'Warning'}: {msg}\")\n    def show_success(msg, title=None): print(f\"‚úÖ {title or 'Success'}: {msg}\")\n    def show_error(msg, title=None): print(f\"‚ùå {title or 'Error'}: {msg}\")\n    def show_hint(msg, title=None): print(f\"üí° {title or 'Hint'}: {msg}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up comprehensive lab progress tracking\ntry:\n    progress = LabProgress(\n        steps=[\n            \"Environment Setup\",\n            \"PDF Processing\", \n            \"Data Ingestion\",\n            \"Vector Index Creation\",\n            \"Agent Tools Setup\",\n            \"LLM Integration\",\n            \"Basic Agent Testing\",\n            \"Memory Implementation\",\n            \"ReAct Agent Enhancement\"\n        ],\n        lab_name=\"Multimodal Agents Workshop\",\n        persist=True\n    )\n    \n    # Set up validation\n    validator = LabValidator(progress_tracker=progress)\n    \n    show_success(\"Lab progress tracking initialized!\")\n    show_info(f\"Workshop: {progress.lab_name}\")\n    show_info(f\"Total steps: {len(progress.steps)}\")\n    \nexcept NameError:\n    show_info(\"Running without progress tracking\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and connecting to MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show step guidance\ntry:\n    progress.show_step_tips(\"Environment Setup\")\nexcept (NameError, AttributeError):\n    show_info(\"Setting up environment and connections...\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style='background-color: #ffebee; border-left: 6px solid #f44336; \n",
       "                padding: 10px; margin: 10px 0; border-radius: 5px;'>\n",
       "        <span style='font-size: 16px; margin-right: 8px;'>‚ùå</span>\n",
       "        Missing environment variables: ['MONGODB_URI', 'SERVERLESS_URL']\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background-color: #e7f3ff; border-left: 6px solid #2196F3; \n",
       "                    padding: 10px; margin: 10px 0; border-radius: 5px;'>\n",
       "            <span style='font-size: 16px; margin-right: 8px;'>‚ÑπÔ∏è</span>\n",
       "            Please set the required environment variables before proceeding\n",
       "            \n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background-color: #ffebee; border-left: 4px solid #f44336; \n",
       "                    padding: 10px; margin: 10px 0; border-radius: 5px;'>\n",
       "            <span style='font-size: 20px; margin-right: 10px;'>‚ùå</span>\n",
       "            <strong style='color: #f44336;'>Type mismatch for 'MONGODB_URI'</strong>\n",
       "            <br><small style='color: #666;'>Expected str, got NoneType</small>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background-color: #ffebee; border-left: 4px solid #f44336; \n",
       "                    padding: 10px; margin: 10px 0; border-radius: 5px;'>\n",
       "            <span style='font-size: 20px; margin-right: 10px;'>‚ùå</span>\n",
       "            <strong style='color: #f44336;'>Type mismatch for 'SERVERLESS_URL'</strong>\n",
       "            <br><small style='color: #666;'>Expected str, got NoneType</small>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Check environment variables\n",
    "required_vars = [\"MONGODB_URI\", \"SERVERLESS_URL\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    show_error(f\"Missing environment variables: {missing_vars}\")\n",
    "    show_info(\"Please set the required environment variables before proceeding\")\n",
    "else:\n",
    "    show_success(\"All required environment variables are set!\")\n",
    "\n",
    "# Validate connection variables\n",
    "try:\n",
    "    validator.validate_variable_exists(\"MONGODB_URI\", {\"MONGODB_URI\": os.getenv(\"MONGODB_URI\")}, str)\n",
    "    validator.validate_variable_exists(\"SERVERLESS_URL\", {\"SERVERLESS_URL\": os.getenv(\"SERVERLESS_URL\")}, str)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB Atlas\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "SERVERLESS_URL = os.getenv(\"SERVERLESS_URL\")\n",
    "LLM_PROVIDER = \"google\"\n",
    "\n",
    "# Initialize MongoDB client\n",
    "try:\n",
    "    mongodb_client = MongoClient(MONGODB_URI)\n",
    "    # Test the connection\n",
    "    result = mongodb_client.admin.command(\"ping\")\n",
    "    \n",
    "    if result.get(\"ok\") == 1:\n",
    "        show_success(\"Successfully connected to MongoDB Atlas! üéâ\")\n",
    "        \n",
    "        # Mark step as complete\n",
    "        try:\n",
    "            progress.mark_done(\"Environment Setup\", score=100, notes=\"MongoDB connection successful\")\n",
    "        except NameError:\n",
    "            pass\n",
    "    else:\n",
    "        show_error(\"MongoDB connection failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Connection error: {e}\")\n",
    "    show_hint(\"Check your connection string and network access settings\", \n",
    "             \"Connection Troubleshooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: PDF Processing\n",
    "\n",
    "Download a research paper and extract pages as images for multimodal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show step guidance\ntry:\n    progress.show_step_tips(\"PDF Processing\")\nexcept (NameError, AttributeError):\n    show_info(\"Processing PDF and extracting images...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory for images\n",
    "Path(\"data/images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "show_info(\"üìö Reference: https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the DeepSeek paper\n",
    "try:\n",
    "    show_info(\"Downloading DeepSeek R1 research paper...\")\n",
    "    response = requests.get(\"https://arxiv.org/pdf/2501.12948\")\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "    \n",
    "    # Get the content of the response\n",
    "    pdf_stream = response.content\n",
    "    show_success(f\"PDF downloaded successfully! Size: {len(pdf_stream)} bytes\")\n",
    "    \n",
    "    # TODO: Open the data in `pdf_stream` as a PDF document\n",
    "    # HINT: Set the `filetype` argument to \"pdf\"\n",
    "    pdf = pymupdf.Document(stream=pdf_stream, filetype=\"pdf\")\n",
    "    \n",
    "    show_success(f\"PDF loaded! Pages: {pdf.page_count}\")\n",
    "    \n",
    "    # Validate PDF processing\n",
    "    try:\n",
    "        validator.validate_variable_exists('pdf', locals(), pymupdf.Document)\n",
    "        validator.validate_custom(\n",
    "            pdf.page_count > 0,\n",
    "            \"PDF has valid page count\",\n",
    "            \"PDF appears to be empty or corrupted\"\n",
    "        )\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"PDF processing failed: {e}\")\n",
    "    show_hint(\"Check your internet connection and try again\", \"Download Issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract pages as images\nfrom tqdm import tqdm\n\ndocs = []\nzoom = 3.0\n\nshow_info(\"üìö Reference: https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap\")\n\ntry:\n    # Set image matrix dimensions\n    mat = pymupdf.Matrix(zoom, zoom)\n    \n    show_info(f\"Extracting {pdf.page_count} pages as images...\")\n    \n    # Track partial progress\n    total_pages = pdf.page_count\n    \n    # Iterate through the pages of the PDF\n    for n in tqdm(range(pdf.page_count), desc=\"Extracting pages\"):\n        temp = {}\n        \n        # TODO: Use the `get_pixmap` method to render the PDF page\n        # HINT: Access the PDF page as pdf[n]\n        pix = pdf[n].get_pixmap(matrix=mat)\n        \n        # Store image locally\n        key = f\"data/images/{n+1}.png\"\n        pix.save(key)\n        \n        # Extract image metadata\n        temp[\"key\"] = key\n        temp[\"width\"] = pix.width\n        temp[\"height\"] = pix.height\n        temp[\"page_number\"] = n + 1\n        docs.append(temp)\n    \n    show_success(f\"Successfully extracted {len(docs)} pages as images!\")\n    show_info(f\"Images saved to: data/images/\")\n    \n    # Mark step complete\n    try:\n        progress.mark_done(\"PDF Processing\", score=95, \n                          notes=f\"Extracted {len(docs)} pages\")\n    except (NameError, AttributeError):\n        pass\n        \nexcept Exception as e:\n    show_error(f\"Image extraction failed: {e}\")\n    show_hint(\"Ensure the data/images directory exists and is writable\", \"File Access\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Ingestion\n",
    "\n",
    "Load pre-generated embeddings and ingest them into MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Generate embeddings (requires Voyage AI API key)\n",
    "show_info(\"‚ÑπÔ∏è Embedding Generation\", \"Optional Step\")\n",
    "show_info(\"\"\"\n",
    "For this workshop, we'll use pre-generated embeddings to save time.\n",
    "If you want to generate your own embeddings, uncomment the code below \n",
    "and add your Voyage AI API key.\n",
    "\n",
    "Follow these steps to get an API key:\n",
    "https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys\n",
    "\"\"\")\n",
    "\n",
    "# Uncomment this section if you have a Voyage AI API key\n",
    "# from voyageai import Client\n",
    "# from PIL import Image\n",
    "# \n",
    "# os.environ[\"VOYAGE_API_KEY\"] = \"your-api-key-here\"\n",
    "# voyageai_client = Client()\n",
    "# \n",
    "# def get_embedding(data, input_type):\n",
    "#     \"\"\"Get Voyage AI embeddings for images and text.\"\"\"\n",
    "#     embedding = voyageai_client.multimodal_embed(\n",
    "#         inputs=[[data]], model=\"voyage-multimodal-3\", input_type=input_type\n",
    "#     ).embeddings[0]\n",
    "#     return embedding\n",
    "# \n",
    "# embedded_docs = []\n",
    "# for doc in tqdm(docs, desc=\"Generating embeddings\"):\n",
    "#     img = Image.open(doc['key'])\n",
    "#     doc[\"embedding\"] = get_embedding(img, \"document\")\n",
    "#     embedded_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Database configuration\n",
    "DB_NAME = \"mongodb_aiewf\"\n",
    "COLLECTION_NAME = \"multimodal_workshop\"\n",
    "\n",
    "# Connect to the collection\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "show_info(f\"Connected to database: {DB_NAME}\")\n",
    "show_info(f\"Using collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-generated embeddings\n",
    "try:\n",
    "    show_info(\"Loading pre-generated embeddings...\")\n",
    "    \n",
    "    with open(\"data/embeddings.json\", \"r\") as data_file:\n",
    "        json_data = data_file.read()\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    show_success(f\"Loaded {len(data)} documents with embeddings\")\n",
    "    \n",
    "    # Validate data structure\n",
    "    try:\n",
    "        validator.validate_custom(\n",
    "            len(data) > 0,\n",
    "            \"Embeddings data loaded successfully\",\n",
    "            \"Embeddings file is empty or invalid\"\n",
    "        )\n",
    "        \n",
    "        # Check if first document has required fields\n",
    "        if data:\n",
    "            required_fields = ['embedding', 'key']\n",
    "            missing_fields = [field for field in required_fields if field not in data[0]]\n",
    "            \n",
    "            validator.validate_custom(\n",
    "                len(missing_fields) == 0,\n",
    "                \"Document structure validation passed\",\n",
    "                f\"Missing required fields: {missing_fields}\"\n",
    "            )\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    show_error(\"Embeddings file not found: data/embeddings.json\")\n",
    "    show_hint(\"Make sure the data/embeddings.json file exists in your working directory\", \n",
    "             \"File Missing\")\n",
    "except Exception as e:\n",
    "    show_error(f\"Failed to load embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data into MongoDB\n",
    "show_info(\"üìö Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many\")\n",
    "\n",
    "try:\n",
    "    # Clear existing documents\n",
    "    delete_result = collection.delete_many({})\n",
    "    show_info(f\"Deleted {delete_result.deleted_count} existing documents\")\n",
    "    \n",
    "    # TODO: Bulk insert documents into the collection\n",
    "    insert_result = collection.insert_many(data)\n",
    "    \n",
    "    # Verify insertion\n",
    "    doc_count = collection.count_documents({})\n",
    "    \n",
    "    show_success(f\"Successfully ingested {doc_count} documents into {COLLECTION_NAME}! üéâ\")\n",
    "    \n",
    "    # Validate ingestion\n",
    "    try:\n",
    "        validator.validate_custom(\n",
    "            doc_count == len(data),\n",
    "            \"All documents ingested successfully\",\n",
    "            f\"Document count mismatch: expected {len(data)}, got {doc_count}\"\n",
    "        )\n",
    "        \n",
    "        progress.mark_done(\"Data Ingestion\", score=100, \n",
    "                          notes=f\"Ingested {doc_count} documents\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Data ingestion failed: {e}\")\n",
    "    show_hint(\"Check your MongoDB connection and permissions\", \"Database Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Vector Search Index Creation\n",
    "\n",
    "Create a vector search index to enable similarity search on our multimodal embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show step guidance\ntry:\n    progress.show_step_tips(\"Vector Index Creation\")\nexcept (NameError, AttributeError):\n    show_info(\"Creating vector search index...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VS_INDEX_NAME = \"vector_index\"\n",
    "\n",
    "# Define vector index configuration\n",
    "model = {\n",
    "    \"name\": VS_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 1024,\n",
    "                \"similarity\": \"cosine\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "show_info(f\"Index configuration: {VS_INDEX_NAME}\")\n",
    "show_info(\"Vector field: embedding\")\n",
    "show_info(\"Dimensions: 1024 (Voyage multimodal)\")\n",
    "show_info(\"Similarity metric: cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector search index\n",
    "show_info(\"üìö Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index\")\n",
    "\n",
    "try:\n",
    "    # Check if index already exists\n",
    "    existing_indexes = list(collection.list_search_indexes())\n",
    "    index_exists = any(idx.get('name') == VS_INDEX_NAME for idx in existing_indexes)\n",
    "    \n",
    "    if index_exists:\n",
    "        show_info(f\"Index '{VS_INDEX_NAME}' already exists\")\n",
    "    else:\n",
    "        show_info(\"Creating vector search index...\")\n",
    "        \n",
    "        # TODO: Create the vector search index\n",
    "        collection.create_search_index(model=model)\n",
    "        \n",
    "        show_success(f\"Vector search index '{VS_INDEX_NAME}' created successfully! üéâ\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"Vector Index Creation\", score=100, \n",
    "                          notes=f\"Index '{VS_INDEX_NAME}' ready\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Index creation failed: {e}\")\n",
    "    show_hint(\"Index creation may take a few minutes. Check Atlas UI to monitor progress\", \n",
    "             \"Index Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index status\n",
    "try:\n",
    "    indexes = list(collection.list_search_indexes())\n",
    "    \n",
    "    show_info(\"Current search indexes:\")\n",
    "    for idx in indexes:\n",
    "        name = idx.get('name', 'Unknown')\n",
    "        status = idx.get('status', 'Unknown')\n",
    "        \n",
    "        if status == 'READY':\n",
    "            show_success(f\"‚úÖ {name}: {status}\")\n",
    "        else:\n",
    "            show_warning(f\"‚è≥ {name}: {status}\")\n",
    "    \n",
    "    # Check if our index is ready\n",
    "    our_index = next((idx for idx in indexes if idx.get('name') == VS_INDEX_NAME), None)\n",
    "    \n",
    "    if our_index and our_index.get('status') == 'READY':\n",
    "        show_success(f\"Index '{VS_INDEX_NAME}' is ready for vector search! üöÄ\")\n",
    "    else:\n",
    "        show_warning(f\"Index '{VS_INDEX_NAME}' is still building. Please wait...\")\n",
    "        show_hint(\"Index creation can take several minutes. Check the Atlas UI for progress.\", \n",
    "                 \"Index Building\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Failed to check index status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Agent Tools Setup\n",
    "\n",
    "Create the vector search tool that our AI agent will use to retrieve relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "show_info(\"üìö Reference: https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_for_question_answering(user_query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve information using vector search to answer a user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image file paths retrieved from vector search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"üîç Searching for: {user_query}\")\n",
    "        \n",
    "        # Embed the user query using our serverless endpoint\n",
    "        response = requests.post(\n",
    "            url=SERVERLESS_URL,\n",
    "            json={\n",
    "                \"task\": \"get_embedding\",\n",
    "                \"data\": {\"input\": user_query, \"input_type\": \"query\"},\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            show_error(f\"Embedding API failed: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        # Extract the embedding from the response\n",
    "        query_embedding = response.json()[\"embedding\"]\n",
    "        show_success(f\"Generated query embedding: {len(query_embedding)} dimensions\")\n",
    "\n",
    "        # TODO: Define aggregation pipeline with $vectorSearch and $project stages\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": VS_INDEX_NAME,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": query_embedding,\n",
    "                    \"numCandidates\": 150,\n",
    "                    \"limit\": 2,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"_id\": 0,\n",
    "                    \"key\": 1,\n",
    "                    \"width\": 1,\n",
    "                    \"height\": 1,\n",
    "                    \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # TODO: Execute the aggregation pipeline\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        \n",
    "        # Extract image keys\n",
    "        keys = [result[\"key\"] for result in results]\n",
    "        scores = [result[\"score\"] for result in results]\n",
    "        \n",
    "        show_success(f\"Found {len(keys)} relevant images\")\n",
    "        for i, (key, score) in enumerate(zip(keys, scores)):\n",
    "            show_info(f\"  {i+1}. {key} (score: {score:.4f})\")\n",
    "        \n",
    "        return keys\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Vector search failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function declaration for Gemini function calling\n",
    "show_info(\"üìö Reference: https://ai.google.dev/gemini-api/docs/function-calling#step_1_define_function_declaration\")\n",
    "\n",
    "# TODO: Define the function declaration\n",
    "get_information_for_question_answering_declaration = {\n",
    "    \"name\": \"get_information_for_question_answering\",\n",
    "    \"description\": \"Retrieve information using vector search to answer a user query.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"user_query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Query string to use for vector search\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"user_query\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "show_success(\"Function declaration created for Gemini integration!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Agent Tools Setup\", score=100, \n",
    "                      notes=\"Vector search tool and function declaration ready\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: LLM Integration\n",
    "\n",
    "Set up Gemini 2.0 Flash with function calling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.types import FunctionCall\n",
    "\n",
    "LLM = \"gemini-2.0-flash\"\n",
    "\n",
    "try:\n",
    "    # Get API key from serverless endpoint\n",
    "    show_info(\"Obtaining Gemini API key...\")\n",
    "    \n",
    "    api_response = requests.post(\n",
    "        url=SERVERLESS_URL, \n",
    "        json={\"task\": \"get_api_key\", \"data\": LLM_PROVIDER}\n",
    "    )\n",
    "    \n",
    "    if api_response.status_code == 200:\n",
    "        api_key = api_response.json()[\"api_key\"]\n",
    "        \n",
    "        # Initialize Gemini client\n",
    "        gemini_client = genai.Client(api_key=api_key)\n",
    "        \n",
    "        show_success(f\"Gemini client initialized with model: {LLM}\")\n",
    "        \n",
    "        # Validate client setup\n",
    "        try:\n",
    "            validator.validate_variable_exists('gemini_client', locals(), genai.Client)\n",
    "        except NameError:\n",
    "            pass\n",
    "    else:\n",
    "        show_error(f\"Failed to get API key: {api_response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"LLM setup failed: {e}\")\n",
    "    show_hint(\"Check your SERVERLESS_URL and network connection\", \"API Key Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generation configuration\n",
    "try:\n",
    "    tools = types.Tool(\n",
    "        function_declarations=[get_information_for_question_answering_declaration]\n",
    "    )\n",
    "    tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)\n",
    "    \n",
    "    show_success(\"Generation configuration created with function calling enabled!\")\n",
    "    show_info(\"Temperature: 0.0 (deterministic responses)\")\n",
    "    show_info(\"Available tools: get_information_for_question_answering\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"LLM Integration\", score=100, \n",
    "                          notes=\"Gemini 2.0 Flash configured with function calling\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Configuration failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Basic Agent Implementation\n",
    "\n",
    "Create the core agent functions for tool selection and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "show_info(\"üìö Reference: https://ai.google.dev/gemini-api/docs/function-calling#step_4_create_user_friendly_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tool(messages: List) -> FunctionCall | None:\n",
    "    \"\"\"\n",
    "    Use an LLM to decide which tool to call.\n",
    "\n",
    "    Args:\n",
    "        messages (List): Messages as a list\n",
    "\n",
    "    Returns:\n",
    "        FunctionCall: Function call object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = [\n",
    "            (\n",
    "                \"You're an AI assistant. Based on the given information, decide which tool to use. \"\n",
    "                \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image. \"\n",
    "                \"Here is the provided information:\\n\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Input to the LLM\n",
    "        contents = system_prompt + messages\n",
    "        \n",
    "        # TODO: Generate response using Gemini\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM, contents=contents, config=tools_config\n",
    "        )\n",
    "        \n",
    "        # Extract and return the function call\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].function_call\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Tool selection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "show_success(\"Tool selection function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(user_query: str, images: List = []) -> str:\n",
    "    \"\"\"\n",
    "    Execute any tools and generate a response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User's query string\n",
    "        images (List): List of image file paths. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Use select_tool to determine if we need to call any tools\n",
    "        tool_call = select_tool([user_query])\n",
    "        \n",
    "        # If a tool call is found and it's our vector search function\n",
    "        if (\n",
    "            tool_call is not None\n",
    "            and tool_call.name == \"get_information_for_question_answering\"\n",
    "        ):\n",
    "            show_info(f\"üõ†Ô∏è Agent calling tool: {tool_call.name}\")\n",
    "            \n",
    "            # TODO: Call the tool with the extracted arguments\n",
    "            tool_images = get_information_for_question_answering(**tool_call.args)\n",
    "            \n",
    "            # Add retrieved images to the input images\n",
    "            images.extend(tool_images)\n",
    "\n",
    "        # Prepare system prompt\n",
    "        system_prompt = (\n",
    "            \"Answer the questions based on the provided context only. \"\n",
    "            \"If the context is not sufficient, say I DON'T KNOW. \"\n",
    "            \"DO NOT use any other information to answer the question.\"\n",
    "        )\n",
    "        \n",
    "        # Prepare contents for the LLM\n",
    "        contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]\n",
    "\n",
    "        # Get the response from the LLM\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(temperature=0.0),\n",
    "        )\n",
    "        \n",
    "        answer = response.text\n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Answer generation failed: {e}\")\n",
    "        return \"I apologize, but I encountered an error while processing your question.\"\n",
    "\n",
    "show_success(\"Answer generation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_agent(user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the agent and display the response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ü§ñ Processing query: {user_query}\")\n",
    "        \n",
    "        response = generate_answer(user_query, images)\n",
    "        \n",
    "        show_success(\"ü§ñ Agent Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"Agent execution function created!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Basic Agent Testing\", score=100, \n",
    "                      notes=\"Agent functions implemented and ready for testing\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with different types of queries\n",
    "show_info(\"üß™ Testing the agent with sample queries...\")\n",
    "\n",
    "# Test 1: Text-based query requiring vector search\n",
    "show_info(\"Test 1: Factual question requiring document search\")\n",
    "execute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Image explanation (if test image exists)\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"data/test.png\"):\n",
    "    show_info(\"Test 2: Image analysis\")\n",
    "    execute_agent(\"Explain the graph in this image:\", [\"data/test.png\"])\n",
    "else:\n",
    "    show_warning(\"Test image not found: data/test.png\")\n",
    "    show_info(\"Test 2: Using extracted PDF page instead\")\n",
    "    if docs:\n",
    "        execute_agent(\"What can you see in this document page?\", [docs[0]['key']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Memory Implementation\n",
    "\n",
    "Add conversational memory to enable multi-turn conversations with context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Set up history collection\n",
    "history_collection = mongodb_client[DB_NAME][\"history\"]\n",
    "\n",
    "show_info(f\"Setting up conversation memory in: {DB_NAME}.history\")\n",
    "show_info(\"üìö Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for efficient session queries\n",
    "try:\n",
    "    # TODO: Create index on session_id field\n",
    "    history_collection.create_index(\"session_id\")\n",
    "    \n",
    "    show_success(\"Session index created for conversation history!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    show_error(f\"Index creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Create chat history document and store it in MongoDB.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID\n",
    "        role (str): Message role, one of 'user' or 'agent'\n",
    "        type (str): Type of message, one of 'text' or 'image'\n",
    "        content (str): Content of the message (text or image path)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Create message document\n",
    "        message = {\n",
    "            \"session_id\": session_id,\n",
    "            \"role\": role,\n",
    "            \"type\": type,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now(),\n",
    "        }\n",
    "        \n",
    "        # TODO: Insert message into history collection\n",
    "        history_collection.insert_one(message)\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Failed to store chat message: {e}\")\n",
    "\n",
    "show_success(\"Chat message storage function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID\n",
    "\n",
    "    Returns:\n",
    "        List: List of messages (text and images)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(\"üìö Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort\")\n",
    "        \n",
    "        # TODO: Query history collection and sort by timestamp\n",
    "        cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "        \n",
    "        messages = []\n",
    "        if cursor:\n",
    "            for msg in cursor:\n",
    "                # If message type is text, append content as is\n",
    "                if msg[\"type\"] == \"text\":\n",
    "                    messages.append(msg[\"content\"])\n",
    "                # If message type is image, open and append the image\n",
    "                elif msg[\"type\"] == \"image\":\n",
    "                    try:\n",
    "                        messages.append(Image.open(msg[\"content\"]))\n",
    "                    except Exception as e:\n",
    "                        show_warning(f\"Could not load image {msg['content']}: {e}\")\n",
    "        \n",
    "        return messages\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Failed to retrieve session history: {e}\")\n",
    "        return []\n",
    "\n",
    "show_success(\"Session history retrieval function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced generate_answer function with memory\n",
    "def generate_answer_with_memory(session_id: str, user_query: str, images: List = []) -> str:\n",
    "    \"\"\"\n",
    "    Execute tools and generate response with conversation memory.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID for conversation tracking\n",
    "        user_query (str): User's query string\n",
    "        images (List): List of image file paths. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Retrieve conversation history\n",
    "        history = retrieve_session_history(session_id)\n",
    "        \n",
    "        show_info(f\"Retrieved {len(history)} previous messages for session {session_id}\")\n",
    "        \n",
    "        # Determine if tools need to be called\n",
    "        tool_call = select_tool(history + [user_query])\n",
    "        \n",
    "        if (\n",
    "            tool_call is not None\n",
    "            and tool_call.name == \"get_information_for_question_answering\"\n",
    "        ):\n",
    "            show_info(f\"üõ†Ô∏è Agent calling tool: {tool_call.name}\")\n",
    "            tool_images = get_information_for_question_answering(**tool_call.args)\n",
    "            images.extend(tool_images)\n",
    "\n",
    "        # Generate response with history context\n",
    "        system_prompt = (\n",
    "            \"Answer the questions based on the provided context only. \"\n",
    "            \"If the context is not sufficient, say I DON'T KNOW. \"\n",
    "            \"DO NOT use any other information to answer the question.\"\n",
    "        )\n",
    "        \n",
    "        contents = (\n",
    "            [system_prompt]\n",
    "            + history\n",
    "            + [user_query]\n",
    "            + [Image.open(image) for image in images]\n",
    "        )\n",
    "        \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(temperature=0.0),\n",
    "        )\n",
    "        \n",
    "        answer = response.text\n",
    "        \n",
    "        # Store conversation in memory\n",
    "        # TODO: Store user query\n",
    "        store_chat_message(session_id, \"user\", \"text\", user_query)\n",
    "        \n",
    "        # TODO: Store image references\n",
    "        for image in images:\n",
    "            store_chat_message(session_id, \"user\", \"image\", image)\n",
    "        \n",
    "        # TODO: Store agent response\n",
    "        store_chat_message(session_id, \"agent\", \"text\", answer)\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Memory-enabled answer generation failed: {e}\")\n",
    "        return \"I apologize, but I encountered an error while processing your question.\"\n",
    "\n",
    "show_success(\"Memory-enabled answer generation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced execute_agent function with memory\n",
    "def execute_agent_with_memory(session_id: str, user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the agent with conversation memory.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID for conversation tracking\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"üß† Session {session_id} - Processing: {user_query}\")\n",
    "        \n",
    "        response = generate_answer_with_memory(session_id, user_query, images)\n",
    "        \n",
    "        show_success(\"ü§ñ Agent Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Memory-enabled agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"Memory-enabled agent execution function created!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Memory Implementation\", score=100, \n",
    "                      notes=\"Conversation memory system implemented\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory-enabled agent\n",
    "show_info(\"üß™ Testing memory-enabled agent...\")\n",
    "\n",
    "# First query in session\n",
    "show_info(\"Test 1: Initial query\")\n",
    "execute_agent_with_memory(\n",
    "    \"session_1\",\n",
    "    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up query to test memory\n",
    "show_info(\"Test 2: Follow-up query to test memory\")\n",
    "execute_agent_with_memory(\n",
    "    \"session_1\",\n",
    "    \"What did I just ask you?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: ReAct Agent Enhancement\n",
    "\n",
    "Implement a ReAct (Reasoning + Acting) agent that can reason about whether it has enough information and iteratively gather more data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_react(user_query: str, images: List = []) -> str:\n",
    "    \"\"\"\n",
    "    Implement a ReAct (Reasoning + Acting) agent.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User's query string\n",
    "        images (List): List of image file paths. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(\"üß† Starting ReAct agent processing...\")\n",
    "        \n",
    "        # Define reasoning prompt\n",
    "        system_prompt = [\n",
    "            (\n",
    "                \"You are an AI assistant. Based on the current information, decide if you have enough to answer the user query, or if you need more information. \"\n",
    "                \"If you have enough information, respond with 'ANSWER: <your answer>'. \"\n",
    "                \"If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise. \"\n",
    "                f\"User query: {user_query}\\n\"\n",
    "                \"Current information:\\n\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Set max iterations to prevent infinite loops\n",
    "        max_iterations = 3\n",
    "        current_iteration = 0\n",
    "        \n",
    "        # Initialize list to accumulate information\n",
    "        current_information = []\n",
    "\n",
    "        # If the user provided images, add them to current information\n",
    "        if len(images) != 0:\n",
    "            current_information.extend([Image.open(image) for image in images])\n",
    "            show_info(f\"Added {len(images)} user-provided images to context\")\n",
    "\n",
    "        # Run the reasoning ‚Üí action loop\n",
    "        while current_iteration < max_iterations:\n",
    "            current_iteration += 1\n",
    "            show_info(f\"üîÑ ReAct Iteration {current_iteration}:\")\n",
    "            \n",
    "            # Generate reasoning and decision\n",
    "            response = gemini_client.models.generate_content(\n",
    "                model=LLM,\n",
    "                contents=system_prompt + current_information,\n",
    "                config=types.GenerateContentConfig(temperature=0.0),\n",
    "            )\n",
    "            \n",
    "            decision = response.text\n",
    "            show_info(f\"üí≠ Agent decision: {decision[:100]}...\")\n",
    "            \n",
    "            # If the agent has the final answer, return it\n",
    "            if \"ANSWER:\" in decision:\n",
    "                final_answer = decision.split(\"ANSWER:\", 1)[1].strip()\n",
    "                show_success(f\"‚úÖ Final answer reached in {current_iteration} iterations\")\n",
    "                return final_answer\n",
    "            \n",
    "            # If the agent decides to use a tool\n",
    "            elif \"TOOL:\" in decision:\n",
    "                tool_query = decision.split(\"TOOL:\", 1)[1].strip()\n",
    "                show_info(f\"üõ†Ô∏è Agent requesting tool with query: {tool_query}\")\n",
    "                \n",
    "                # Use tool selection to get the function call\n",
    "                tool_call = select_tool([tool_query])\n",
    "                \n",
    "                if (\n",
    "                    tool_call is not None\n",
    "                    and tool_call.name == \"get_information_for_question_answering\"\n",
    "                ):\n",
    "                    show_info(f\"üìä Calling vector search with: {tool_call.args}\")\n",
    "                    \n",
    "                    # Call the tool and add results to current information\n",
    "                    tool_images = get_information_for_question_answering(**tool_call.args)\n",
    "                    \n",
    "                    if tool_images:\n",
    "                        new_images = [Image.open(image) for image in tool_images]\n",
    "                        current_information.extend(new_images)\n",
    "                        show_success(f\"‚ûï Added {len(new_images)} retrieved images to context\")\n",
    "                    else:\n",
    "                        show_warning(\"No relevant images found\")\n",
    "                        current_information.append(\"No relevant visual information found for this query.\")\n",
    "                else:\n",
    "                    show_warning(\"Tool selection failed or returned unexpected tool\")\n",
    "                    current_information.append(\"Tool call failed.\")\n",
    "            else:\n",
    "                show_warning(\"Agent response didn't contain ANSWER or TOOL directive\")\n",
    "                current_information.append(\"Unable to determine next action.\")\n",
    "        \n",
    "        # If we've exhausted iterations without a final answer\n",
    "        show_warning(f\"‚ö†Ô∏è Reached maximum iterations ({max_iterations}) without final answer\")\n",
    "        return \"I apologize, but I couldn't find a definitive answer after exploring the available information. Please try rephrasing your question or asking for more specific details.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"ReAct agent failed: {e}\")\n",
    "        return \"I apologize, but I encountered an error while processing your question with the ReAct approach.\"\n",
    "\n",
    "show_success(\"ReAct agent implementation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_react_agent(user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the ReAct agent.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ü¶∏‚Äç‚ôÄÔ∏è ReAct Agent Processing: {user_query}\")\n",
    "        \n",
    "        response = generate_answer_react(user_query, images)\n",
    "        \n",
    "        show_success(\"ü§ñ ReAct Agent Final Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"ReAct agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"ReAct agent execution function created!\")\n",
    "\n",
    "# Mark final step complete\n",
    "try:\n",
    "    progress.mark_done(\"ReAct Agent Enhancement\", score=100, \n",
    "                      notes=\"ReAct reasoning and acting agent implemented\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ReAct agent\n",
    "show_info(\"üß™ Testing ReAct agent with iterative reasoning...\")\n",
    "\n",
    "# Test 1: Question requiring document search\n",
    "show_info(\"Test 1: Complex factual question\")\n",
    "execute_react_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Image analysis (if available)\n",
    "if os.path.exists(\"data/test.png\"):\n",
    "    show_info(\"Test 2: Image analysis with ReAct\")\n",
    "    execute_react_agent(\"Explain the graph in this image:\", [\"data/test.png\"])\n",
    "else:\n",
    "    show_info(\"Test 2: Document page analysis with ReAct\")\n",
    "    if docs:\n",
    "        execute_react_agent(\"What technical concepts are discussed in this document page?\", [docs[0]['key']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Workshop Complete!\n",
    "\n",
    "Congratulations! You've successfully built a comprehensive multimodal AI agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final progress summary\n",
    "try:\n",
    "    show_success(\"üéì Workshop Completed Successfully!\")\n",
    "    \n",
    "    # Display final progress\n",
    "    progress.display_progress(detailed=True)\n",
    "    \n",
    "    # Show completion statistics\n",
    "    completion_rate = progress.get_completion_rate()\n",
    "    avg_score = progress.get_average_score()\n",
    "    \n",
    "    show_info(f\"üìä Overall Completion: {completion_rate:.1f}%\")\n",
    "    if avg_score:\n",
    "        show_info(f\"üìà Average Score: {avg_score:.1f}/100\")\n",
    "    \n",
    "    # Show what was accomplished\n",
    "    show_success(\"\"\"\n",
    "    üöÄ What You've Built:\n",
    "    \n",
    "    ‚úÖ PDF processing pipeline for multimodal content\n",
    "    ‚úÖ MongoDB Atlas vector search integration\n",
    "    ‚úÖ AI agent with function calling capabilities\n",
    "    ‚úÖ Conversational memory system\n",
    "    ‚úÖ ReAct (Reasoning + Acting) agent architecture\n",
    "    ‚úÖ End-to-end multimodal AI application\n",
    "    \"\"\")\n",
    "    \n",
    "    # Next steps\n",
    "    show_info(\"\"\"\n",
    "    üéØ Next Steps:\n",
    "    \n",
    "    ‚Ä¢ Experiment with different types of documents and queries\n",
    "    ‚Ä¢ Modify the agent to work with your own data\n",
    "    ‚Ä¢ Add more sophisticated reasoning capabilities\n",
    "    ‚Ä¢ Integrate with web interfaces or chat applications\n",
    "    ‚Ä¢ Explore other multimodal models and embeddings\n",
    "    \"\"\")\n",
    "    \n",
    "except NameError:\n",
    "    show_success(\"üéì Workshop completed successfully!\")\n",
    "    show_info(\"All agent implementations are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export progress analytics\n",
    "try:\n",
    "    if hasattr(progress, 'export_analytics_json'):\n",
    "        analytics_file = progress.export_analytics_json()\n",
    "        show_success(f\"üìÑ Progress analytics exported to: {analytics_file}\")\n",
    "        \n",
    "        # Show summary\n",
    "        summary = progress.get_analytics_summary()\n",
    "        if summary:\n",
    "            show_info(f\"‚è±Ô∏è Total session time: {summary.get('session_duration', 'N/A')} seconds\")\n",
    "            show_info(f\"üìù Total interactions: {summary.get('total_events', 'N/A')}\")\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "show_success(\"Thank you for completing the Multimodal Agents Workshop! üôè\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}