{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ai4 Multimodal Agents Workshop - No API Key Required! ðŸš€\n",
    "\n",
    "This version of the multimodal agents workshop uses **pre-created embeddings** and does not require VoyageAI API keys, making it accessible to everyone!\n",
    "\n",
    "**Workshop Overview:**\n",
    "- Build a multimodal AI agent that can analyze documents and images\n",
    "- Use MongoDB Atlas Vector Search for retrieval \n",
    "- Implement function calling with Gemini 2.0 Flash\n",
    "- Add memory and ReAct reasoning capabilities\n",
    "- **NEW**: Works without VoyageAI API keys using pre-created embeddings\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "- Load and use pre-created multimodal embeddings\n",
    "- Set up MongoDB Atlas vector search indexes\n",
    "- Build an AI agent with tool calling capabilities\n",
    "- Implement session-based memory for conversational agents\n",
    "- Create a ReAct (Reasoning + Acting) agent architecture\n",
    "- **NEW**: Understand embedding fallback strategies for production systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress tracking and lab utilities\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Force load from development source if available\n",
    "dev_path = \"/Users/michael.lynn/code/mongodb/developer-days/jupyter-utils/jupyter-lab-progress\"\n",
    "if os.path.exists(dev_path) and dev_path not in sys.path:\n",
    "    sys.path.insert(0, dev_path)\n",
    "\n",
    "# Remove any cached modules\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('jupyter_lab_progress')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "try:\n",
    "    from jupyter_lab_progress import (\n",
    "        LabProgress, LabValidator, show_info, show_warning, \n",
    "        show_success, show_error, show_hint\n",
    "    )\n",
    "    show_success(\"Progress tracking libraries loaded successfully! ðŸŽ‰\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import progress tracking: {e}\")\n",
    "    print(\"Installing basic fallbacks...\")\n",
    "    def show_info(msg, title=None): print(f\"â„¹ï¸ {title or 'Info'}: {msg}\")\n",
    "    def show_warning(msg, title=None): print(f\"âš ï¸ {title or 'Warning'}: {msg}\")\n",
    "    def show_success(msg, title=None): print(f\"âœ… {title or 'Success'}: {msg}\")\n",
    "    def show_error(msg, title=None): print(f\"âŒ {title or 'Error'}: {msg}\")\n",
    "    def show_hint(msg, title=None): print(f\"ðŸ’¡ {title or 'Hint'}: {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up comprehensive lab progress tracking\n",
    "try:\n",
    "    progress = LabProgress(\n",
    "        steps=[\n",
    "            \"Environment Setup\",\n",
    "            \"Fallback Embeddings Setup\", \n",
    "            \"Pre-created Embeddings Loading\",\n",
    "            \"Data Ingestion\",\n",
    "            \"Vector Index Creation\",\n",
    "            \"Agent Tools Setup\", \n",
    "            \"LLM Integration\",\n",
    "            \"Basic Agent Testing\",\n",
    "            \"Memory Implementation\",\n",
    "            \"ReAct Agent Enhancement\"\n",
    "        ],\n",
    "        lab_name=\"Multimodal Agents - No API Keys Required\",\n",
    "        persist=True\n",
    "    )\n",
    "    \n",
    "    # Set up validation\n",
    "    validator = LabValidator(progress_tracker=progress)\n",
    "    \n",
    "    show_success(\"Lab progress tracking initialized!\")\n",
    "    show_info(f\"Workshop: {progress.lab_name}\")\n",
    "    show_info(f\"Total steps: {len(progress.steps)}\")\n",
    "    \n",
    "except NameError:\n",
    "    show_info(\"Running without progress tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and connecting to MongoDB Atlas.\n",
    "\n",
    "**Note**: This version only requires `MONGODB_URI` and `GOOGLE_API_KEY` - no VoyageAI API key needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Environment Setup\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Setting up environment and connections...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load environment variables from .env file (only MongoDB and Google API keys required)\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "if env_path.exists():\n",
    "    # Load variables from .env\n",
    "    with open(env_path) as f:\n",
    "        for line in f:\n",
    "            if '=' in line and not line.strip().startswith('#'):\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value.strip('\\\"\\'')\n",
    "    show_info(\"Loaded environment variables from .env file\")\n",
    "else:\n",
    "    show_warning(\".env file not found, using environment variables from system\")\n",
    "\n",
    "# Check required environment variables (VoyageAI not required!)\n",
    "required_vars = [\"MONGODB_URI\", \"GOOGLE_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    show_error(f\"âŒ Missing required environment variables: {missing_vars}\")\n",
    "    show_hint(\"Create a .env file with:\\nMONGODB_URI=your_mongodb_connection_string\\nGOOGLE_API_KEY=your_google_api_key\")\n",
    "    raise ValueError(f\"Missing required environment variables: {missing_vars}\")\n",
    "\n",
    "show_success(\"All required environment variables are set!\")\n",
    "show_info(\"âœ“ MONGODB_URI: Available\")\n",
    "show_info(\"âœ“ GOOGLE_API_KEY: Available\")\n",
    "show_success(\"âœ… No VoyageAI API key required for this workshop!\")\n",
    "\n",
    "# Validate connection variables\n",
    "try:\n",
    "    validator.validate_variable_exists(\"MONGODB_URI\", {\"MONGODB_URI\": os.getenv(\"MONGODB_URI\")}, str)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB Atlas\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "SERVERLESS_URL = os.getenv(\"SERVERLESS_URL\")  # Optional fallback\n",
    "LLM_PROVIDER = \"google\"\n",
    "\n",
    "# Initialize MongoDB client\n",
    "try:\n",
    "    mongodb_client = MongoClient(MONGODB_URI)\n",
    "    # Test the connection\n",
    "    result = mongodb_client.admin.command(\"ping\")\n",
    "    \n",
    "    if result.get(\"ok\") == 1:\n",
    "        show_success(\"Successfully connected to MongoDB Atlas! ðŸŽ‰\")\n",
    "        \n",
    "        # Mark step as complete\n",
    "        try:\n",
    "            progress.mark_done(\"Environment Setup\", score=100, notes=\"MongoDB connection successful\")\n",
    "        except NameError:\n",
    "            pass\n",
    "    else:\n",
    "        show_error(\"MongoDB connection failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Connection error: {e}\")\n",
    "    show_hint(\"Check your connection string and network access settings\", \n",
    "             \"Connection Troubleshooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fallback Embeddings Setup\n",
    "\n",
    "Set up a lightweight embedding system that works without VoyageAI API keys.\n",
    "\n",
    "**Strategy**:\n",
    "- **Document embeddings**: Load from pre-created `data/embeddings.json` file\n",
    "- **Query embeddings**: Use sentence-transformers as a lightweight fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Fallback Embeddings Setup\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Setting up fallback embedding system...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Install sentence-transformers if not available (fallback for query embeddings)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    show_success(\"sentence-transformers available for query embedding fallback!\")\n",
    "    # Use a lightweight model for query embeddings\n",
    "    query_encoder = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, 384-dim embeddings\n",
    "    show_success(\"Query embedding model loaded!\")\n",
    "except ImportError:\n",
    "    show_warning(\"sentence-transformers not available, installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"sentence-transformers\"])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    query_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    show_success(\"sentence-transformers installed and ready!\")\n",
    "\n",
    "# Normalize vector function (MongoDB doesn't auto-normalize)\n",
    "def normalize_vector(v):\n",
    "    \"\"\"Normalize a vector to unit length.\"\"\"\n",
    "    norm = np.linalg.norm(v)\n",
    "    return v / norm if norm > 0 else v\n",
    "\n",
    "show_success(\"Vector normalization utility ready\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Fallback Embeddings Setup\", score=100, \n",
    "                      notes=\"Fallback embedding system configured\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pre-created Embeddings Loading\n",
    "\n",
    "Load the pre-created embeddings instead of generating them with VoyageAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Pre-created Embeddings Loading\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Loading pre-created embeddings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding generation function with fallback\n",
    "def generate_embedding_fallback(data, input_type=\"document\"):\n",
    "    \"\"\"\n",
    "    Generate embedding using fallback methods - no VoyageAI API required!\n",
    "    \n",
    "    Args:\n",
    "        data: PIL Image or text string\n",
    "        input_type: \"document\" or \"query\"\n",
    "    \n",
    "    Returns:\n",
    "        list: Normalized embedding vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(data, Image.Image):\n",
    "            # For images, we'll use pre-created embeddings (loaded separately)\n",
    "            show_warning(\"Image embedding requested, but we use pre-created embeddings for images\")\n",
    "            return None\n",
    "        else:\n",
    "            # For text queries, use sentence-transformers\n",
    "            embedding = query_encoder.encode(str(data))\n",
    "            \n",
    "            # Note: sentence-transformers produces 384-dim embeddings, \n",
    "            # but our pre-created embeddings are 1024-dim\n",
    "            # We'll need to handle this dimension mismatch\n",
    "            \n",
    "            # Pad to 1024 dimensions to match document embeddings\n",
    "            if len(embedding) < 1024:\n",
    "                padding = np.zeros(1024 - len(embedding))\n",
    "                embedding = np.concatenate([embedding, padding])\n",
    "            elif len(embedding) > 1024:\n",
    "                embedding = embedding[:1024]  # Truncate if too long\n",
    "            \n",
    "            # Normalize the embedding\n",
    "            normalized_embedding = normalize_vector(np.array(embedding)).tolist()\n",
    "            \n",
    "            return normalized_embedding\n",
    "            \n",
    "    except Exception as e:\n",
    "        show_error(f\"Fallback embedding generation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "show_success(\"Fallback embedding generation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-created embeddings\n",
    "embeddings_file = Path(\"data/embeddings.json\")\n",
    "\n",
    "if not embeddings_file.exists():\n",
    "    show_error(f\"Pre-created embeddings file not found: {embeddings_file}\")\n",
    "    show_hint(\"Make sure the data/embeddings.json file exists in your working directory\", \n",
    "             \"Missing File\")\n",
    "    raise FileNotFoundError(f\"Required embeddings file not found: {embeddings_file}\")\n",
    "\n",
    "try:\n",
    "    with open(embeddings_file, \"r\") as f:\n",
    "        embedded_docs = json.load(f)\n",
    "    \n",
    "    show_success(f\"Loaded {len(embedded_docs)} pre-created document embeddings!\")\n",
    "    \n",
    "    # Analyze the structure\n",
    "    if embedded_docs:\n",
    "        sample = embedded_docs[0]\n",
    "        show_info(f\"Sample document keys: {list(sample.keys())}\")\n",
    "        show_info(f\"Embedding dimensions: {len(sample.get('embedding', []))}\")\n",
    "        \n",
    "        # Add missing page_number field if needed\n",
    "        for i, doc in enumerate(embedded_docs):\n",
    "            if 'page_number' not in doc:\n",
    "                # Extract page number from image filename (e.g., \"data/images/1.png\" -> 1)\n",
    "                key = doc.get('key', '')\n",
    "                if 'images/' in key:\n",
    "                    try:\n",
    "                        page_num = int(key.split('/')[-1].split('.')[0])\n",
    "                        doc['page_number'] = page_num\n",
    "                    except:\n",
    "                        doc['page_number'] = i + 1\n",
    "                else:\n",
    "                    doc['page_number'] = i + 1\n",
    "        \n",
    "        show_success(\"Added missing page_number fields to documents\")\n",
    "    \n",
    "    # Validate embeddings\n",
    "    try:\n",
    "        validator.validate_custom(\n",
    "            len(embedded_docs) > 0,\n",
    "            \"Pre-created embeddings loaded successfully\",\n",
    "            \"No embeddings found in the file\"\n",
    "        )\n",
    "        \n",
    "        progress.mark_done(\"Pre-created Embeddings Loading\", score=100, \n",
    "                          notes=f\"Loaded {len(embedded_docs)} pre-created embeddings\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Failed to load pre-created embeddings: {e}\")\n",
    "    show_hint(\"Check that the embeddings.json file is valid JSON\", \"File Format\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Data Ingestion\n",
    "\n",
    "Ingest the pre-created embeddings into MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration  \n",
    "DB_NAME = \"mongodb_aiewf\"\n",
    "COLLECTION_NAME = \"multimodal_workshop_fallback\"\n",
    "\n",
    "# Connect to the collection\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "show_info(f\"Connected to database: {DB_NAME}\")\n",
    "show_info(f\"Using collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data into MongoDB\n",
    "show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many\")\n",
    "\n",
    "try:\n",
    "    # Clear existing documents\n",
    "    delete_result = collection.delete_many({})\n",
    "    show_info(f\"Deleted {delete_result.deleted_count} existing documents\")\n",
    "    \n",
    "    # Bulk insert documents into the collection\n",
    "    insert_result = collection.insert_many(embedded_docs)\n",
    "    \n",
    "    # Verify insertion\n",
    "    doc_count = collection.count_documents({})\n",
    "    \n",
    "    show_success(f\"Successfully ingested {doc_count} documents into {COLLECTION_NAME}! ðŸŽ‰\")\n",
    "    \n",
    "    # Validate ingestion\n",
    "    try:\n",
    "        validator.validate_custom(\n",
    "            doc_count == len(embedded_docs),\n",
    "            \"All documents ingested successfully\",\n",
    "            f\"Document count mismatch: expected {len(embedded_docs)}, got {doc_count}\"\n",
    "        )\n",
    "        \n",
    "        progress.mark_done(\"Data Ingestion\", score=100, \n",
    "                          notes=f\"Ingested {doc_count} documents\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Data ingestion failed: {e}\")\n",
    "    show_hint(\"Check your MongoDB connection and permissions\", \"Database Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Vector Search Index Creation\n",
    "\n",
    "Create a vector search index to enable similarity search on our pre-created embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Vector Index Creation\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Creating vector search index...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VS_INDEX_NAME = \"vector_index_fallback\"\n",
    "\n",
    "# Define vector index configuration\n",
    "model = {\n",
    "    \"name\": VS_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 1024,  # Pre-created embeddings are 1024-dim\n",
    "                \"similarity\": \"cosine\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "show_info(f\"Index configuration: {VS_INDEX_NAME}\")\n",
    "show_info(\"Vector field: embedding\")\n",
    "show_info(\"Dimensions: 1024 (Pre-created embeddings)\")\n",
    "show_info(\"Similarity metric: cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector search index\n",
    "show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index\")\n",
    "\n",
    "try:\n",
    "    # Check if index already exists\n",
    "    existing_indexes = list(collection.list_search_indexes())\n",
    "    index_exists = any(idx.get('name') == VS_INDEX_NAME for idx in existing_indexes)\n",
    "    \n",
    "    if index_exists:\n",
    "        show_info(f\"Index '{VS_INDEX_NAME}' already exists\")\n",
    "    else:\n",
    "        show_info(\"Creating vector search index...\")\n",
    "        \n",
    "        # Create the vector search index\n",
    "        collection.create_search_index(model=model)\n",
    "        \n",
    "        show_success(f\"Vector search index '{VS_INDEX_NAME}' created successfully! ðŸŽ‰\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"Vector Index Creation\", score=100, \n",
    "                          notes=f\"Index '{VS_INDEX_NAME}' ready\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Index creation failed: {e}\")\n",
    "    show_hint(\"Index creation may take a few minutes. Check Atlas UI to monitor progress\", \n",
    "             \"Index Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index status\n",
    "try:\n",
    "    indexes = list(collection.list_search_indexes())\n",
    "    \n",
    "    show_info(\"Current search indexes:\")\n",
    "    for idx in indexes:\n",
    "        name = idx.get('name', 'Unknown')\n",
    "        status = idx.get('status', 'Unknown')\n",
    "        \n",
    "        if status == 'READY':\n",
    "            show_success(f\"âœ… {name}: {status}\")\n",
    "        else:\n",
    "            show_warning(f\"â³ {name}: {status}\")\n",
    "    \n",
    "    # Check if our index is ready\n",
    "    our_index = next((idx for idx in indexes if idx.get('name') == VS_INDEX_NAME), None)\n",
    "    \n",
    "    if our_index and our_index.get('status') == 'READY':\n",
    "        show_success(f\"Index '{VS_INDEX_NAME}' is ready for vector search! ðŸš€\")\n",
    "    else:\n",
    "        show_warning(f\"Index '{VS_INDEX_NAME}' is still building. Please wait...\")\n",
    "        show_hint(\"Index creation can take several minutes. Check the Atlas UI for progress.\", \n",
    "                 \"Index Building\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Failed to check index status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Agent Tools Setup\n",
    "\n",
    "Create the vector search tool using fallback embedding generation for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "show_info(\"ðŸ“š Reference: https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_for_question_answering(user_query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve information using vector search to answer a user query.\n",
    "    Uses fallback embedding generation for queries (no API keys required).\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of image file paths retrieved from vector search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ” Searching for: {user_query}\")\n",
    "        \n",
    "        # Generate query embedding using fallback method\n",
    "        query_embedding = generate_embedding_fallback(user_query, input_type=\"query\")\n",
    "        \n",
    "        if not query_embedding:\n",
    "            show_error(\"Failed to generate query embedding\")\n",
    "            return []\n",
    "        \n",
    "        show_success(f\"Generated query embedding: {len(query_embedding)} dimensions\")\n",
    "\n",
    "        # Define aggregation pipeline with $vectorSearch and $project stages\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": VS_INDEX_NAME,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": query_embedding,\n",
    "                    \"numCandidates\": 150,  # Higher for better recall\n",
    "                    \"limit\": 2,  # Top results\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"_id\": 0,\n",
    "                    \"key\": 1,\n",
    "                    \"width\": 1,\n",
    "                    \"height\": 1,\n",
    "                    \"page_number\": 1,\n",
    "                    \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Execute the aggregation pipeline\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        \n",
    "        # Extract image keys and scores\n",
    "        keys = [result[\"key\"] for result in results]\n",
    "        scores = [result[\"score\"] for result in results]\n",
    "        \n",
    "        show_success(f\"Found {len(keys)} relevant images\")\n",
    "        for i, (key, score) in enumerate(zip(keys, scores)):\n",
    "            show_info(f\"  {i+1}. {key} (score: {score:.4f})\")\n",
    "        \n",
    "        return keys\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Vector search failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function declaration for Gemini function calling\n",
    "show_info(\"ðŸ“š Reference: https://ai.google.dev/gemini-api/docs/function-calling#step_1_define_function_declaration\")\n",
    "\n",
    "# Define the function declaration\n",
    "get_information_for_question_answering_declaration = {\n",
    "    \"name\": \"get_information_for_question_answering\",\n",
    "    \"description\": \"Retrieve information using vector search to answer a user query. Uses pre-created embeddings and fallback query encoding.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"user_query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Query string to use for vector search\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"user_query\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "show_success(\"Function declaration created for Gemini integration!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Agent Tools Setup\", score=100, \n",
    "                      notes=\"Vector search tool with fallback embedding ready\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: LLM Integration\n",
    "\n",
    "Set up Gemini 2.0 Flash with function calling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.genai.types import FunctionCall\n",
    "\n",
    "LLM = \"gemini-2.0-flash\"\n",
    "\n",
    "try:\n",
    "    # Use GOOGLE_API_KEY from environment (required)\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    \n",
    "    # Initialize Gemini client\n",
    "    gemini_client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    show_success(f\"Gemini client initialized with model: {LLM}\")\n",
    "    show_info(\"Using GOOGLE_API_KEY from environment\")\n",
    "    \n",
    "    # Validate client setup\n",
    "    try:\n",
    "        validator.validate_variable_exists('gemini_client', locals(), genai.Client)\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"LLM setup failed: {e}\")\n",
    "    show_hint(\"Check your GOOGLE_API_KEY in .env file\", \"API Key Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generation configuration\n",
    "try:\n",
    "    tools = types.Tool(\n",
    "        function_declarations=[get_information_for_question_answering_declaration]\n",
    "    )\n",
    "    tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)\n",
    "    \n",
    "    show_success(\"Generation configuration created with function calling enabled!\")\n",
    "    show_info(\"Temperature: 0.0 (deterministic responses)\")\n",
    "    show_info(\"Available tools: get_information_for_question_answering\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"LLM Integration\", score=100, \n",
    "                          notes=\"Gemini 2.0 Flash configured with function calling\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Configuration failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Basic Agent Implementation\n",
    "\n",
    "Create the core agent functions for tool selection and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info(\"ðŸ“š Reference: https://ai.google.dev/gemini-api/docs/function-calling#step_4_create_user_friendly_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tool(messages: List) -> FunctionCall | None:\n",
    "    \"\"\"\n",
    "    Use an LLM to decide which tool to call.\n",
    "\n",
    "    Args:\n",
    "        messages (List): Messages as a list\n",
    "\n",
    "    Returns:\n",
    "        FunctionCall: Function call object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = [\n",
    "            (\n",
    "                \"You're an AI assistant. Based on the given information, decide which tool to use. \"\n",
    "                \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image. \"\n",
    "                \"Here is the provided information:\\n\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Input to the LLM\n",
    "        contents = system_prompt + messages\n",
    "        \n",
    "        # Generate response using Gemini\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM, contents=contents, config=tools_config\n",
    "        )\n",
    "        \n",
    "        # Extract and return the function call\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].function_call\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Tool selection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "show_success(\"Tool selection function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(user_query: str, images: List = []) -> str:\n",
    "    \"\"\"\n",
    "    Execute any tools and generate a response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User's query string\n",
    "        images (List): List of image file paths. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use select_tool to determine if we need to call any tools\n",
    "        tool_call = select_tool([user_query])\n",
    "        \n",
    "        # If a tool call is found and it's our vector search function\n",
    "        if (\n",
    "            tool_call is not None\n",
    "            and tool_call.name == \"get_information_for_question_answering\"\n",
    "        ):\n",
    "            show_info(f\"ðŸ› ï¸ Agent calling tool: {tool_call.name}\")\n",
    "            \n",
    "            # Call the tool with the extracted arguments\n",
    "            tool_images = get_information_for_question_answering(**tool_call.args)\n",
    "            \n",
    "            # Add retrieved images to the input images\n",
    "            images.extend(tool_images)\n",
    "\n",
    "        # Prepare system prompt\n",
    "        system_prompt = (\n",
    "            \"Answer the questions based on the provided context only. \"\n",
    "            \"If the context is not sufficient, say I DON'T KNOW. \"\n",
    "            \"DO NOT use any other information to answer the question.\"\n",
    "        )\n",
    "        \n",
    "        # Load and validate images\n",
    "        valid_images = []\n",
    "        for img_path in images:\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                valid_images.append(img)\n",
    "            except Exception as e:\n",
    "                show_warning(f\"Failed to load image {img_path}: {e}\")\n",
    "        \n",
    "        # Prepare contents for the LLM\n",
    "        contents = [system_prompt] + [user_query] + valid_images\n",
    "\n",
    "        # Get the response from the LLM\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(temperature=0.0),\n",
    "        )\n",
    "        \n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Answer generation failed: {e}\")\n",
    "        return \"I apologize, but I encountered an error while processing your question.\"\n",
    "\n",
    "show_success(\"Answer generation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_agent(user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the agent and display the response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ¤– Processing query: {user_query}\")\n",
    "        \n",
    "        response = generate_answer(user_query, images)\n",
    "        \n",
    "        show_success(\"ðŸ¤– Agent Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"Agent execution function created!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Basic Agent Testing\", score=100, \n",
    "                      notes=\"Agent functions with fallback embeddings ready\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with different types of queries\n",
    "show_info(\"ðŸ§ª Testing the agent with sample queries...\")\n",
    "\n",
    "# Test 1: Text-based query requiring vector search\n",
    "show_info(\"Test 1: Factual question requiring document search\")\n",
    "execute_agent(\"What is the Pass@1 accuracy of DeepSeek R1 on AIME 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Simple image analysis if we have the original images\n",
    "import os\n",
    "\n",
    "# Check if we have access to any of the original images\n",
    "test_images = [\"data/images/1.png\", \"data/images/2.png\", \"data/images/3.png\"]\n",
    "available_images = [img for img in test_images if os.path.exists(img)]\n",
    "\n",
    "if available_images:\n",
    "    show_info(\"Test 2: Document page analysis\")\n",
    "    execute_agent(\"What can you see in this document page?\", [available_images[0]])\n",
    "else:\n",
    "    show_warning(\"No original document images available for testing\")\n",
    "    show_hint(\"The pre-created embeddings reference images that may not exist locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Memory Implementation\n",
    "\n",
    "Add conversational memory to enable multi-turn conversations with context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Set up history collection\n",
    "history_collection = mongodb_client[DB_NAME][\"history_fallback\"]\n",
    "\n",
    "show_info(f\"Setting up conversation memory in: {DB_NAME}.history_fallback\")\n",
    "show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for efficient session queries\n",
    "try:\n",
    "    # Create index on session_id field\n",
    "    history_collection.create_index(\"session_id\")\n",
    "    \n",
    "    show_success(\"Session index created for conversation history!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    show_error(f\"Index creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Create chat history document and store it in MongoDB.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID\n",
    "        role (str): Message role, one of 'user' or 'agent'\n",
    "        type (str): Type of message, one of 'text' or 'image'\n",
    "        content (str): Content of the message (text or image path)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create message document\n",
    "        message = {\n",
    "            \"session_id\": session_id,\n",
    "            \"role\": role,\n",
    "            \"type\": type,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now(),\n",
    "        }\n",
    "        \n",
    "        # Insert message into history collection\n",
    "        history_collection.insert_one(message)\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Failed to store chat message: {e}\")\n",
    "\n",
    "show_success(\"Chat message storage function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID\n",
    "\n",
    "    Returns:\n",
    "        List: List of messages (text and images)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort\")\n",
    "        \n",
    "        # Query history collection and sort by timestamp\n",
    "        cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "        \n",
    "        messages = []\n",
    "        if cursor:\n",
    "            for msg in cursor:\n",
    "                # If message type is text, append content as is\n",
    "                if msg[\"type\"] == \"text\":\n",
    "                    messages.append(msg[\"content\"])\n",
    "                # If message type is image, open and append the image\n",
    "                elif msg[\"type\"] == \"image\":\n",
    "                    try:\n",
    "                        messages.append(Image.open(msg[\"content\"]))\n",
    "                    except Exception as e:\n",
    "                        show_warning(f\"Could not load image {msg['content']}: {e}\")\n",
    "        \n",
    "        return messages\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Failed to retrieve session history: {e}\")\n",
    "        return []\n",
    "\n",
    "show_success(\"Session history retrieval function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced generate_answer function with memory\n",
    "def generate_answer_with_memory(session_id: str, user_query: str, images: List = []) -> str:\n",
    "    \"\"\"\n",
    "    Execute tools and generate response with conversation memory.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID for conversation tracking\n",
    "        user_query (str): User's query string\n",
    "        images (List): List of image file paths. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve conversation history\n",
    "        history = retrieve_session_history(session_id)\n",
    "        \n",
    "        show_info(f\"Retrieved {len(history)} previous messages for session {session_id}\")\n",
    "        \n",
    "        # Determine if tools need to be called\n",
    "        tool_call = select_tool(history + [user_query])\n",
    "        \n",
    "        if (\n",
    "            tool_call is not None\n",
    "            and tool_call.name == \"get_information_for_question_answering\"\n",
    "        ):\n",
    "            show_info(f\"ðŸ› ï¸ Agent calling tool: {tool_call.name}\")\n",
    "            tool_images = get_information_for_question_answering(**tool_call.args)\n",
    "            images.extend(tool_images)\n",
    "\n",
    "        # Generate response with history context\n",
    "        system_prompt = (\n",
    "            \"Answer the questions based on the provided context only. \"\n",
    "            \"If the context is not sufficient, say I DON'T KNOW. \"\n",
    "            \"DO NOT use any other information to answer the question.\"\n",
    "        )\n",
    "        \n",
    "        contents = (\n",
    "            [system_prompt]\n",
    "            + history\n",
    "            + [user_query]\n",
    "            + [Image.open(image) for image in images if os.path.exists(image)]\n",
    "        )\n",
    "        \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(temperature=0.0),\n",
    "        )\n",
    "        \n",
    "        answer = response.text\n",
    "        \n",
    "        # Store conversation in memory\n",
    "        # Store user query\n",
    "        store_chat_message(session_id, \"user\", \"text\", user_query)\n",
    "        \n",
    "        # Store image references\n",
    "        for image in images:\n",
    "            store_chat_message(session_id, \"user\", \"image\", image)\n",
    "        \n",
    "        # Store agent response\n",
    "        store_chat_message(session_id, \"agent\", \"text\", answer)\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Memory-enabled answer generation failed: {e}\")\n",
    "        return \"I apologize, but I encountered an error while processing your question.\"\n",
    "\n",
    "show_success(\"Memory-enabled answer generation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced execute_agent function with memory\n",
    "def execute_agent_with_memory(session_id: str, user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the agent with conversation memory.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID for conversation tracking\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ§  Session {session_id} - Processing: {user_query}\")\n",
    "        \n",
    "        response = generate_answer_with_memory(session_id, user_query, images)\n",
    "        \n",
    "        show_success(\"ðŸ¤– Agent Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Memory-enabled agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"Memory-enabled agent execution function created!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Memory Implementation\", score=100, \n",
    "                      notes=\"Conversation memory system implemented\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory-enabled agent\n",
    "show_info(\"ðŸ§ª Testing memory-enabled agent...\")\n",
    "\n",
    "# First query in session\n",
    "show_info(\"Test 1: Initial query\")\n",
    "execute_agent_with_memory(\n",
    "    \"session_fallback_1\",\n",
    "    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up query to test memory\n",
    "show_info(\"Test 2: Follow-up query to test memory\")\n",
    "execute_agent_with_memory(\n",
    "    \"session_fallback_1\",\n",
    "    \"What did I just ask you?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: ReAct Agent Enhancement\n",
    "\n",
    "Implement a ReAct (Reasoning + Acting) agent that can reason about whether it has enough information and iteratively gather more data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_react(user_query: str, images: List = []) -> str:\n",
    "    \"\"\"\n",
    "    Implement a ReAct (Reasoning + Acting) agent with fallback embeddings.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User's query string\n",
    "        images (List): List of image file paths. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(\"ðŸ§  Starting ReAct agent processing with fallback embeddings...\")\n",
    "        \n",
    "        # Define reasoning prompt\n",
    "        system_prompt = [\n",
    "            (\n",
    "                \"You are an AI assistant with access to pre-created document embeddings for search. \"\n",
    "                \"Based on the current information, decide if you have enough to answer the user query, or if you need more information. \"\n",
    "                \"If you have enough information, respond with 'ANSWER: <your answer>'. \"\n",
    "                \"If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise. \"\n",
    "                f\"User query: {user_query}\\n\"\n",
    "                \"Current information:\\n\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Set max iterations to prevent infinite loops\n",
    "        max_iterations = 3\n",
    "        current_iteration = 0\n",
    "        \n",
    "        # Initialize list to accumulate information\n",
    "        current_information = []\n",
    "\n",
    "        # If the user provided images, add them to current information\n",
    "        if len(images) != 0:\n",
    "            valid_images = []\n",
    "            for image in images:\n",
    "                if os.path.exists(image):\n",
    "                    valid_images.append(Image.open(image))\n",
    "            current_information.extend(valid_images)\n",
    "            show_info(f\"Added {len(valid_images)} user-provided images to context\")\n",
    "\n",
    "        # Run the reasoning â†’ action loop\n",
    "        while current_iteration < max_iterations:\n",
    "            current_iteration += 1\n",
    "            show_info(f\"ðŸ”„ ReAct Iteration {current_iteration}:\")\n",
    "            \n",
    "            # Generate reasoning and decision\n",
    "            response = gemini_client.models.generate_content(\n",
    "                model=LLM,\n",
    "                contents=system_prompt + current_information,\n",
    "                config=types.GenerateContentConfig(temperature=0.0),\n",
    "            )\n",
    "            \n",
    "            decision = response.text\n",
    "            show_info(f\"ðŸ’­ Agent decision: {decision[:100]}...\")\n",
    "            \n",
    "            # If the agent has the final answer, return it\n",
    "            if \"ANSWER:\" in decision:\n",
    "                final_answer = decision.split(\"ANSWER:\", 1)[1].strip()\n",
    "                show_success(f\"âœ… Final answer reached in {current_iteration} iterations\")\n",
    "                return final_answer\n",
    "            \n",
    "            # If the agent decides to use a tool\n",
    "            elif \"TOOL:\" in decision:\n",
    "                tool_query = decision.split(\"TOOL:\", 1)[1].strip()\n",
    "                show_info(f\"ðŸ› ï¸ Agent requesting tool with query: {tool_query}\")\n",
    "                \n",
    "                # Use tool selection to get the function call\n",
    "                tool_call = select_tool([tool_query])\n",
    "                \n",
    "                if (\n",
    "                    tool_call is not None\n",
    "                    and tool_call.name == \"get_information_for_question_answering\"\n",
    "                ):\n",
    "                    show_info(f\"ðŸ“Š Calling fallback-powered vector search with: {tool_call.args}\")\n",
    "                    \n",
    "                    # Call the tool and add results to current information\n",
    "                    tool_images = get_information_for_question_answering(**tool_call.args)\n",
    "                    \n",
    "                    if tool_images:\n",
    "                        new_images = []\n",
    "                        for image in tool_images:\n",
    "                            if os.path.exists(image):\n",
    "                                new_images.append(Image.open(image))\n",
    "                        current_information.extend(new_images)\n",
    "                        show_success(f\"âž• Added {len(new_images)} retrieved images to context\")\n",
    "                    else:\n",
    "                        show_warning(\"No relevant images found\")\n",
    "                        current_information.append(\"No relevant visual information found for this query.\")\n",
    "                else:\n",
    "                    show_warning(\"Tool selection failed or returned unexpected tool\")\n",
    "                    current_information.append(\"Tool call failed.\")\n",
    "            else:\n",
    "                show_warning(\"Agent response didn't contain ANSWER or TOOL directive\")\n",
    "                current_information.append(\"Unable to determine next action.\")\n",
    "        \n",
    "        # If we've exhausted iterations without a final answer\n",
    "        show_warning(f\"âš ï¸ Reached maximum iterations ({max_iterations}) without final answer\")\n",
    "        return \"I apologize, but I couldn't find a definitive answer after exploring the available information. Please try rephrasing your question or asking for more specific details.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"ReAct agent failed: {e}\")\n",
    "        return \"I apologize, but I encountered an error while processing your question with the ReAct approach.\"\n",
    "\n",
    "show_success(\"ReAct agent with fallback embeddings completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_react_agent(user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the ReAct agent.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ¦¸â€â™€ï¸ ReAct Agent Processing: {user_query}\")\n",
    "        \n",
    "        response = generate_answer_react(user_query, images)\n",
    "        \n",
    "        show_success(\"ðŸ¤– ReAct Agent Final Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"ReAct agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"ReAct agent execution function created!\")\n",
    "\n",
    "# Mark final step complete\n",
    "try:\n",
    "    progress.mark_done(\"ReAct Agent Enhancement\", score=100, \n",
    "                      notes=\"ReAct reasoning and acting agent with fallback embeddings implemented\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ReAct agent\n",
    "show_info(\"ðŸ§ª Testing ReAct agent with fallback embeddings...\")\n",
    "\n",
    "# Test 1: Question requiring document search\n",
    "show_info(\"Test 1: Complex factual question\")\n",
    "execute_react_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Document analysis if images are available\n",
    "if available_images:\n",
    "    show_info(\"Test 2: Document page analysis with ReAct\")\n",
    "    execute_react_agent(\"What technical concepts are discussed in this document page?\", [available_images[0]])\n",
    "else:\n",
    "    show_warning(\"No document pages available for ReAct testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Workshop Complete!\n",
    "\n",
    "Congratulations! You've successfully built a comprehensive multimodal AI agent system **without requiring VoyageAI API keys**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final progress summary\n",
    "try:\n",
    "    show_success(\"ðŸŽ“ No-API-Key Workshop Completed Successfully!\")\n",
    "    \n",
    "    # Display final progress\n",
    "    progress.display_progress(detailed=True)\n",
    "    \n",
    "    # Show completion statistics\n",
    "    completion_rate = progress.get_completion_rate()\n",
    "    avg_score = progress.get_average_score()\n",
    "    \n",
    "    show_info(f\"ðŸ“Š Overall Completion: {completion_rate:.1f}%\")\n",
    "    if avg_score:\n",
    "        show_info(f\"ðŸ“ˆ Average Score: {avg_score:.1f}/100\")\n",
    "    \n",
    "    # Show what was accomplished\n",
    "    show_success(\"\"\"\n",
    "    ðŸš€ What You've Built WITHOUT API Keys:\n",
    "    \n",
    "    âœ… Pre-created embeddings loading system\n",
    "    âœ… Fallback query embedding generation\n",
    "    âœ… MongoDB Atlas vector search integration\n",
    "    âœ… AI agent with function calling capabilities\n",
    "    âœ… Conversational memory system\n",
    "    âœ… ReAct (Reasoning + Acting) agent architecture\n",
    "    âœ… Production-ready fallback strategies\n",
    "    âœ… Cost-effective multimodal AI application\n",
    "    âœ… Educational understanding of embedding systems\n",
    "    \"\"\")\n",
    "    \n",
    "    # Next steps\n",
    "    show_info(\"\"\"\n",
    "    ðŸŽ¯ Next Steps and Learning:\n",
    "    \n",
    "    â€¢ Compare performance vs full VoyageAI integration\n",
    "    â€¢ Experiment with different sentence-transformer models\n",
    "    â€¢ Implement better dimension matching strategies\n",
    "    â€¢ Add support for custom embedding generation\n",
    "    â€¢ Explore hybrid retrieval approaches\n",
    "    â€¢ Build your own embedding generation pipeline\n",
    "    â€¢ Consider upgrading to full API access for production\n",
    "    \"\"\")\n",
    "    \n",
    "except NameError:\n",
    "    show_success(\"ðŸŽ“ No-API-Key Workshop completed successfully!\")\n",
    "    show_info(\"All agent implementations with fallback embeddings are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export progress analytics\n",
    "try:\n",
    "    if hasattr(progress, 'export_analytics_json'):\n",
    "        analytics_file = progress.export_analytics_json()\n",
    "        show_success(f\"ðŸ“„ Progress analytics exported to: {analytics_file}\")\n",
    "        \n",
    "        # Show summary\n",
    "        summary = progress.get_analytics_summary()\n",
    "        if summary:\n",
    "            show_info(f\"â±ï¸ Total session time: {summary.get('session_duration', 'N/A')} seconds\")\n",
    "            show_info(f\"ðŸ“ Total interactions: {summary.get('total_events', 'N/A')}\")\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "show_success(\"Thank you for completing the Multimodal Agents Workshop - No API Keys Required! ðŸ™\")\n",
    "show_info(\"ðŸŽ¯ Key Learning: You've mastered embedding fallback strategies for accessible AI!\")\n",
    "show_info(\"ðŸ“š Consider upgrading to VoyageAI or similar services for production applications\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}