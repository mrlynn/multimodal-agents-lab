{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM8rg08YhqZe"
      },
      "source": [
        "# Step 1: Setup prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pymongo import MongoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXLWCWEghuOX"
      },
      "outputs": [],
      "source": [
        "# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here\n",
        "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
        "# Initialize a MongoDB Python client\n",
        "mongodb_client = MongoClient(MONGODB_URI)\n",
        "# Check the connection to the server\n",
        "mongodb_client.admin.command(\"ping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SERVERLESS_URL = os.getenv(\"SERVERLESS_URL\")\n",
        "LLM_PROVIDER = \"google\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUf3jtFzO4-V"
      },
      "source": [
        "# Step 2: Read PDF from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymupdf\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the DeepSeek paper\n",
        "response = requests.get(\"https://arxiv.org/pdf/2501.12948\")\n",
        "if response.status_code != 200:\n",
        "    raise ValueError(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
        "# Get the content of the response\n",
        "pdf_stream = response.content\n",
        "# Open the data in `pdf_stream` as a PDF document.\n",
        "# HINT: Set the `filetype` argument to \"pdf\".\n",
        "pdf = pymupdf.Document(stream=pdf_stream, filetype=\"pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Store PDF images locally and extract metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zoom = 3.0\n",
        "# Set image matrix dimensions\n",
        "mat = pymupdf.Matrix(zoom, zoom)\n",
        "# Iterate through the pages of the PDF\n",
        "for n in tqdm(range(pdf.page_count)):\n",
        "    temp = {}\n",
        "    # Use the `get_pixmap` method to render the PDF page as a matrix of pixels as specified by the variable `mat`\n",
        "    # HINT: Access the PDF page as pdf[n]\n",
        "    pix = pdf[n].get_pixmap(matrix=mat)\n",
        "    # Store image locally\n",
        "    key = f\"data/images/{n+1}.png\"\n",
        "    pix.save(key)\n",
        "    # Extract image metadata to be stored in MongoDB\n",
        "    temp[\"key\"] = key\n",
        "    temp[\"width\"] = pix.width\n",
        "    temp[\"height\"] = pix.height\n",
        "    docs.append(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Generate image embeddings\n",
        "\n",
        "Uncomment this section only if you are generating embedding using your own Voyage AI API key.\n",
        "\n",
        "Follow the steps [here](https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys) to obtain a Voyage AI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from voyageai import Client\n",
        "# from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Set Voyage AI API Key\n",
        "# os.environ[\"VOYAGE_API_KEY\"] = \"your-api-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# voyageai_client = Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_embedding(data, input_type):\n",
        "#     \"\"\"\n",
        "#     Get Voyage AI embeddings for images and text.\n",
        "\n",
        "#     Args:\n",
        "#         data: An image or text to embed\n",
        "#         input_type: Input type, either \"document\" or \"query\"\n",
        "\n",
        "#     Returns: Embeddings as a list\n",
        "#     \"\"\"\n",
        "#     embedding = voyageai_client.multimodal_embed(\n",
        "#         inputs=[[data]], model=\"voyage-multimodal-3\", input_type=input_type\n",
        "#     ).embeddings[0]\n",
        "#     return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedded_docs = []\n",
        "# for doc in tqdm(docs):\n",
        "#     # Open the image from file\n",
        "#     img = Image.open(f\"{doc['key']}\")\n",
        "#     # Add the embeddings to the document\n",
        "#     doc[\"embedding\"] = get_embedding(img, \"document\")\n",
        "#     embedded_docs.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Write embeddings and metadata to MongoDB\n",
        "\n",
        "In this step, we are ingesting a dataset with multimodal embeddings pre-generated, into MongoDB. \n",
        "\n",
        "If you would like to understand how to the embedding process works, uncomment and work through the code in Step 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Database name\n",
        "DB_NAME = \"mongodb_aiewf\"\n",
        "# Name of the collection to insert documents into\n",
        "COLLECTION_NAME = \"multimodal_workshop\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to the collection\n",
        "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read data from local file\n",
        "with open(\"data/embeddings.json\", \"r\") as data_file:\n",
        "    json_data = data_file.read()\n",
        "data = json.loads(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete existing documents from the `collection` collection\n",
        "collection.delete_many({})\n",
        "print(f\"Deleted existing documents from the {COLLECTION_NAME} collection.\")\n",
        "# Bulk insert documents in `data`, into the `collection` collection.\n",
        "collection.insert_many(data)\n",
        "print(\n",
        "    f\"{collection.count_documents({})} documents ingested into the {COLLECTION_NAME} collection.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6: Create a vector search index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VS_INDEX_NAME = \"vector_index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create vector index definition specifying:\n",
        "# path: Path to the embeddings field\n",
        "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
        "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
        "model = {\n",
        "    \"name\": VS_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 1024,\n",
        "                \"similarity\": \"cosine\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a vector search index with the above `model` for the `collection` collection\n",
        "collection.create_search_index(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that the index is in READY status before proceeding\n",
        "list(collection.list_search_indexes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZfheX5FiIhU"
      },
      "source": [
        "# Step 7: Create agent tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to Basic Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_information_for_question_answering(user_query: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve information using vector search to answer a user query.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "    str: The retrieved information formatted as a string.\n",
        "    \"\"\"\n",
        "    # Embed the user query using our serverless endpoint\n",
        "    response = requests.post(\n",
        "        url=SERVERLESS_URL,\n",
        "        json={\n",
        "            \"task\": \"get_embedding\",\n",
        "            \"data\": {\"input\": user_query, \"input_type\": \"query\"},\n",
        "        },\n",
        "    )\n",
        "    # Extract the embedding from the response\n",
        "    query_embedding = response.json()[\"embedding\"]\n",
        "\n",
        "    # Define an aggregation pipeline consisting of a $vectorSearch stage followed by a $project stage\n",
        "    # Set the number of candidates to 150 and only return the top 2 documents from the vector search\n",
        "    # In the $project stage, exclude the `_id` field, include these fields: `key`, `width`, `height`, and the `vectorSearchScore`\n",
        "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": VS_INDEX_NAME,\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"numCandidates\": 150,\n",
        "                \"limit\": 2,\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,\n",
        "                \"key\": 1,\n",
        "                \"width\": 1,\n",
        "                \"height\": 1,\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
        "            }\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Execute the aggregation `pipeline` against the `collection` collection and store the results in `results`\n",
        "    results = collection.aggregate(pipeline)\n",
        "    # Get images from local storage\n",
        "    keys = [result[\"key\"] for result in results]\n",
        "    print(f\"Keys: {keys}\")\n",
        "    return keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_1_define_function_declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the function declaration for the `get_information_for_question_answering` function\n",
        "get_information_for_question_answering_declaration = {\n",
        "    \"name\": \"get_information_for_question_answering\",\n",
        "    \"description\": \"Retrieve information using vector search to answer a user query.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"user_query\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Query string to use for vector search\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"user_query\"],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 8: Instantiate the Gemini client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM = \"gemini-2.0-flash\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = requests.post(\n",
        "    url=SERVERLESS_URL, json={\"task\": \"get_api_key\", \"data\": LLM_PROVIDER}\n",
        ").json()[\"api_key\"]\n",
        "gemini_client = genai.Client(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 9: Create generation config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a generation config with the `get_information_for_question_answering_declaration` function declaration and `temperature` set to 0.0\n",
        "tools = types.Tool(\n",
        "    function_declarations=[get_information_for_question_answering_declaration]\n",
        ")\n",
        "tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 10: Define a function for tool selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.genai.types import FunctionCall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_4_create_user_friendly_response_with_function_result_and_call_the_model_again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_tool(messages: List) -> FunctionCall | None:\n",
        "    \"\"\"\n",
        "    Use an LLM to decide which tool to call\n",
        "\n",
        "    Args:\n",
        "        messages (List): Messages as a list\n",
        "\n",
        "    Returns:\n",
        "        functionCall: Function call object consisting of the tool name and arguments\n",
        "    \"\"\"\n",
        "    system_prompt = [\n",
        "        (\n",
        "            \"You're an AI assistant. Based on the given information, decide which tool to use.\"\n",
        "            \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image.\"\n",
        "            \"Here is the provided information:\\n\"\n",
        "        )\n",
        "    ]\n",
        "    # Input to the LLM\n",
        "    contents = system_prompt + messages\n",
        "    # Use the `gemini_client`, `LLM`, `contents` and `tools_config` defined previously to generate a response using Gemini\n",
        "    response = gemini_client.models.generate_content(\n",
        "        model=LLM, contents=contents, config=tools_config\n",
        "    )\n",
        "    # Extract and return the function call from the response\n",
        "    return response.candidates[0].content.parts[0].function_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 10: Define a function to execute tools and generate responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_3_execute_set_light_values_function_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(user_query: str, images: List = []) -> str:\n",
        "    \"\"\"\n",
        "    Execute any tools and generate a response\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's query string\n",
        "        images (List): List of filepaths. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response\n",
        "    \"\"\"\n",
        "    # Use the `select_tool` function above to get the tool config\n",
        "    tool_call = select_tool([user_query])\n",
        "    # If a tool call is found and the name is `get_information_for_question_answering`\n",
        "    if (\n",
        "        tool_call is not None\n",
        "        and tool_call.name == \"get_information_for_question_answering\"\n",
        "    ):\n",
        "        print(f\"Agent: Calling tool: {tool_call.name}\")\n",
        "        # Call the tool with the arguments extracted by the LLM\n",
        "        tool_images = get_information_for_question_answering(**tool_call.args)\n",
        "        # Add images return by the tool to the list of input images if any\n",
        "        images.extend(tool_images)\n",
        "\n",
        "    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n",
        "    # Pass the system prompt, user query, and content retrieved using vector search (`images`) as input to the LLM\n",
        "    contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]\n",
        "\n",
        "    # Get the response from the LLM\n",
        "    response = gemini_client.models.generate_content(\n",
        "        model=LLM,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(temperature=0.0),\n",
        "    )\n",
        "    answer = response.text\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 11: Define a function to execute the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_agent(user_query: str, images: List = []) -> None:\n",
        "    \"\"\"\n",
        "    Execute the agent.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User query\n",
        "        images (List, optional): List of filepaths. Defaults to [].\n",
        "    \"\"\"\n",
        "    response = generate_answer(user_query, images)\n",
        "    print(\"Agent:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the agent with a text input\n",
        "execute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the agent with an image input\n",
        "execute_agent(\"Explain the graph in this image:\", [\"data/test.png\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 12: Add memory to the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the history collection\n",
        "history_collection = mongodb_client[DB_NAME][\"history\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an index on `session_id` on the `history_collection` collection\n",
        "history_collection.create_index(\"session_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:\n",
        "    \"\"\"\n",
        "    Create chat history document and store it in MongoDB\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "        role (str): Message role, one of `human` or `agent`.\n",
        "        type (str): Type of message, one of `text` or `image`.\n",
        "        content (str): Content of the message. For images, this is the image key.\n",
        "    \"\"\"\n",
        "    # Create a message object with `session_id`, `role`, `type`, `content` and `timestamp` fields\n",
        "    # `timestamp` should be set the current timestamp\n",
        "    message = {\n",
        "        \"session_id\": session_id,\n",
        "        \"role\": role,\n",
        "        \"type\": type,\n",
        "        \"content\": content,\n",
        "        \"timestamp\": datetime.now(),\n",
        "    }\n",
        "    # Insert the `message` into the `history_collection` collection\n",
        "    history_collection.insert_one(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_session_history(session_id: str) -> List:\n",
        "    \"\"\"\n",
        "    Retrieve chat history for a particular session.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "\n",
        "    Returns:\n",
        "        List: List of messages. Can be a combination of text and images.\n",
        "    \"\"\"\n",
        "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
        "    # Sort the results in increasing order of the values in `timestamp` field\n",
        "    cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
        "    messages = []\n",
        "    if cursor:\n",
        "        for msg in cursor:\n",
        "            # Is the message type is `text`, append the content as is\n",
        "            if msg[\"type\"] == \"text\":\n",
        "                messages.append(msg[\"content\"])\n",
        "            # If message type is `image`, open the image\n",
        "            elif msg[\"type\"] == \"image\":\n",
        "                messages.append(Image.open(msg[\"content\"]))\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(session_id: str, user_query: str, images: List = []) -> str:\n",
        "    \"\"\"\n",
        "    Execute any tools and generate a response\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "        user_query (str): User's query string\n",
        "        images (List): List of filepaths. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response\n",
        "    \"\"\"\n",
        "    # Retrieve past conversation history for the specified `session_id` using the `retrieve_session_history` method\n",
        "    history = retrieve_session_history(session_id)\n",
        "    # Determine if any additional tools need to be called\n",
        "    tool_call = select_tool(history + [user_query])\n",
        "    if (\n",
        "        tool_call is not None\n",
        "        and tool_call.name == \"get_information_for_question_answering\"\n",
        "    ):\n",
        "        print(f\"Agent: Calling tool: {tool_call.name}\")\n",
        "        # Call the tool with the arguments extracted by the LLM\n",
        "        tool_images = get_information_for_question_answering(**tool_call.args)\n",
        "        # Add images return by the tool to the list of input images if any\n",
        "        images.extend(tool_images)\n",
        "\n",
        "    # Pass the system prompt, conversation history, user query and retrieved context (`images`) to the LLM to generate an answer\n",
        "    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n",
        "    contents = (\n",
        "        [system_prompt]\n",
        "        + history\n",
        "        + [user_query]\n",
        "        + [Image.open(image) for image in images]\n",
        "    )\n",
        "    # Get a response from the LLM\n",
        "    response = gemini_client.models.generate_content(\n",
        "        model=LLM,\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(temperature=0.0),\n",
        "    )\n",
        "    answer = response.text\n",
        "    # Write the current user query to memory using the `store_chat_message` function\n",
        "    # The `role` for user queries is \"user\" and `type` is \"text\"\n",
        "    store_chat_message(session_id, \"user\", \"text\", user_query)\n",
        "    # Write the filepaths of input/retrieved images to memory using the store_chat_message` function\n",
        "    # The `role` for these is \"user\" and `type` is \"image\"\n",
        "    for image in images:\n",
        "        store_chat_message(session_id, \"user\", \"image\", image)\n",
        "    # Write the LLM generated response to memory\n",
        "    # The `role` for these is \"agent\" and `type` is \"text\"\n",
        "    store_chat_message(session_id, \"agent\", \"text\", answer)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_agent(session_id: str, user_query: str, images: List = []) -> None:\n",
        "    \"\"\"\n",
        "    Execute the agent.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID\n",
        "        user_query (str): User query\n",
        "        images (List, optional): List of filepaths. Defaults to [].\n",
        "    \"\"\"\n",
        "    response = generate_answer(session_id, user_query, images)\n",
        "    print(\"Agent:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_agent(\n",
        "    \"1\",\n",
        "    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Follow-up question to make sure chat history is being used.\n",
        "execute_agent(\n",
        "    \"1\",\n",
        "    \"What did I just ask you?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¦¸â€â™€ï¸ Update to ReAct agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(user_query: str, images: List = []) -> str:\n",
        "    \"\"\"\n",
        "    Implement a ReAct agent\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's query string\n",
        "        images (List): List of filepaths. Defaults to [].\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response\n",
        "    \"\"\"\n",
        "    # Define reasoning prompt\n",
        "    system_prompt = [\n",
        "        (\n",
        "            \"You are an AI assistant. Based on the current information, decide if you have enough to answer the user query, or if you need more information.\"\n",
        "            \"If you have enough information, respond with 'ANSWER: <your answer>'.\"\n",
        "            \"If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise.\"\n",
        "            f\"User query: {user_query}\\n\"\n",
        "            \"Current information:\\n\"\n",
        "        )\n",
        "    ]\n",
        "    # Set max iterations\n",
        "    max_iterations = 3\n",
        "    current_iteration = 0\n",
        "    # Initialize list to accumulate tool outcomes etc.\n",
        "    current_information = []\n",
        "\n",
        "    # If the user input has images, add them to `current_information`\n",
        "    if len(images) != 0:\n",
        "        current_information.extend([Image.open(image) for image in images])\n",
        "\n",
        "    # Run the reasoning -> action taking loop for `max_iterations` number of iterations\n",
        "    while current_iteration < max_iterations:\n",
        "        current_iteration += 1\n",
        "        print(f\"Iteration {current_iteration}:\")\n",
        "        # Generate action -> final answer/tool call\n",
        "        response = gemini_client.models.generate_content(\n",
        "            model=LLM,\n",
        "            contents=system_prompt + current_information,\n",
        "            config=types.GenerateContentConfig(temperature=0.0),\n",
        "        )\n",
        "        answer = response.text\n",
        "        print(f\"Agent: {answer}\")\n",
        "        # If the agent has the final answer, return it\n",
        "        if \"ANSWER\" in answer:\n",
        "            return answer\n",
        "        # If the agent decides to call a tool\n",
        "        else:\n",
        "            # determine which tool to call\n",
        "            tool_call = select_tool([answer])\n",
        "            if (\n",
        "                tool_call is not None\n",
        "                and tool_call.name == \"get_information_for_question_answering\"\n",
        "            ):\n",
        "                print(f\"Agent: Calling tool: {tool_call.name}\")\n",
        "                # Call the tool with the arguments extracted by the LLM\n",
        "                tool_images = get_information_for_question_answering(**tool_call.args)\n",
        "                # Add images return by the tool to the list of input images if any\n",
        "                current_information.extend([Image.open(image) for image in tool_images])\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_agent(user_query: str, images: List = []) -> None:\n",
        "    \"\"\"\n",
        "    Execute the agent.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User query\n",
        "        images (List, optional): List of filepaths. Defaults to [].\n",
        "    \"\"\"\n",
        "    response = generate_answer(user_query, images)\n",
        "    print(\"Agent:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "execute_agent(\"Explain the graph in this image:\", [\"data/test.png\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RM8rg08YhqZe",
        "UUf3jtFzO4-V",
        "Sm5QZdshwJLN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09dcf4ce88064f11980bbefaad1ebc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39563df9477648398456675ec51075aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f4353368efbd4c3891f805ddc3d05e1b",
            "value": "Downloadingâ€‡data:â€‡100%"
          }
        },
        "164d16df28d24ab796b7c9cf85174800": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e4af5b420242b7a6b74a18cad98961",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dff65b579f0746ffae8739ecb0aa5a41",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "20d693a09c534414a5c4c0dd58cf94ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "278513c5a8b04a24b1823d38107f1e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e196b6d30746578e137c50b661f946",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ced7f9d61e06442a960dcda95852048e",
            "value": "â€‡102M/102Mâ€‡[00:06&lt;00:00,â€‡20.6MB/s]"
          }
        },
        "30fe0bcd02cb47f3ba23bb480e2eaaea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373ed3b6307741859ab297c270cf42c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39563df9477648398456675ec51075aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41056c822b9d44559147d2b21416b956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a43c349d171e469c8cc94d48060f775b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_373ed3b6307741859ab297c270cf42c8",
            "value": "â€‡50000/0â€‡[00:04&lt;00:00,â€‡12390.43â€‡examples/s]"
          }
        },
        "62e196b6d30746578e137c50b661f946": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dbfebff68ff45628da832fac5233c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_164d16df28d24ab796b7c9cf85174800",
              "IPY_MODEL_e70e0d317f1e4e73bd95349ed1510cce",
              "IPY_MODEL_41056c822b9d44559147d2b21416b956"
            ],
            "layout": "IPY_MODEL_b1929fb112174c0abcd8004f6be0f880"
          }
        },
        "95e4af5b420242b7a6b74a18cad98961": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a43c349d171e469c8cc94d48060f775b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1929fb112174c0abcd8004f6be0f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebfba144ba6418092df949783f93455": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09dcf4ce88064f11980bbefaad1ebc75",
              "IPY_MODEL_f2bd7bda4d0c4d93b88e53aeb4e1b62d",
              "IPY_MODEL_278513c5a8b04a24b1823d38107f1e50"
            ],
            "layout": "IPY_MODEL_d3941c633788427abb858b21e285088f"
          }
        },
        "ced7f9d61e06442a960dcda95852048e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d17d8c8f45ee44cd87dcd787c05dbdc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3941c633788427abb858b21e285088f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff65b579f0746ffae8739ecb0aa5a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e70e0d317f1e4e73bd95349ed1510cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f73ae771c24645c79fd41409a8fc7b34",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20d693a09c534414a5c4c0dd58cf94ed",
            "value": 1
          }
        },
        "f2bd7bda4d0c4d93b88e53aeb4e1b62d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fe0bcd02cb47f3ba23bb480e2eaaea",
            "max": 102202622,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d17d8c8f45ee44cd87dcd787c05dbdc3",
            "value": 102202622
          }
        },
        "f4353368efbd4c3891f805ddc3d05e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f73ae771c24645c79fd41409a8fc7b34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
