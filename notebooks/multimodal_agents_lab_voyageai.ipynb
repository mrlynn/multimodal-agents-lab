{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Agents Workshop with VoyageAI Python Client\n",
    "\n",
    "This version of the multimodal agents workshop uses the new VoyageAI Python client library for simplified embedding generation and MongoDB Atlas Vector Search integration.\n",
    "\n",
    "**Workshop Overview:**\n",
    "- Build a multimodal AI agent that can analyze documents and images\n",
    "- Use MongoDB Atlas Vector Search for retrieval\n",
    "- Implement function calling with Gemini 2.0 Flash\n",
    "- Add memory and ReAct reasoning capabilities\n",
    "- **NEW**: Streamlined VoyageAI integration with the official Python client\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "- Process PDFs and extract images for multimodal search\n",
    "- Set up MongoDB Atlas vector search indexes\n",
    "- Build an AI agent with tool calling capabilities\n",
    "- Implement session-based memory for conversational agents\n",
    "- Create a ReAct (Reasoning + Acting) agent architecture\n",
    "- **NEW**: Use the VoyageAI Python client for production-ready embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize progress tracking and lab utilities\nimport sys\nimport os\n\ntry:\n    from jupyter_lab_progress import (\n        LabProgress, LabValidator, show_info, show_warning, \n        show_success, show_error, show_hint\n    )\n    show_success(\"Progress tracking libraries loaded successfully! ðŸŽ‰\")\nexcept ImportError as e:\n    print(f\"Warning: Could not import progress tracking: {e}\")\n    print(\"Installing basic fallbacks...\")\n    def show_info(msg, title=None): print(f\"â„¹ï¸ {title or 'Info'}: {msg}\")\n    def show_warning(msg, title=None): print(f\"âš ï¸ {title or 'Warning'}: {msg}\")\n    def show_success(msg, title=None): print(f\"âœ… {title or 'Success'}: {msg}\")\n    def show_error(msg, title=None): print(f\"âŒ {title or 'Error'}: {msg}\")\n    def show_hint(msg, title=None): print(f\"ðŸ’¡ {title or 'Hint'}: {msg}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up comprehensive lab progress tracking\n",
    "try:\n",
    "    progress = LabProgress(\n",
    "        steps=[\n",
    "            \"Environment Setup\",\n",
    "            \"VoyageAI Client Setup\",\n",
    "            \"PDF Processing\", \n",
    "            \"Embedding Generation\",\n",
    "            \"Data Ingestion\",\n",
    "            \"Vector Index Creation\",\n",
    "            \"Agent Tools Setup\",\n",
    "            \"LLM Integration\",\n",
    "            \"Basic Agent Testing\",\n",
    "            \"Memory Implementation\",\n",
    "            \"ReAct Agent Enhancement\"\n",
    "        ],\n",
    "        lab_name=\"Multimodal Agents with VoyageAI Client\",\n",
    "        persist=True\n",
    "    )\n",
    "    \n",
    "    # Set up validation\n",
    "    validator = LabValidator(progress_tracker=progress)\n",
    "    \n",
    "    show_success(\"Lab progress tracking initialized!\")\n",
    "    show_info(f\"Workshop: {progress.lab_name}\")\n",
    "    show_info(f\"Total steps: {len(progress.steps)}\")\n",
    "    \n",
    "except NameError:\n",
    "    show_info(\"Running without progress tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and connecting to MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Environment Setup\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Setting up environment and connections...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pymongo import MongoClient\n\n# Load environment variables from .env file (required)\nfrom pathlib import Path\n\nenv_path = Path('.') / '.env'\nif not env_path.exists():\n    raise FileNotFoundError(\n        \"âŒ .env file is required! Please create a .env file with:\\n\"\n        \"MONGODB_URI=your_mongodb_connection_string\\n\"\n        \"GOOGLE_API_KEY=your_google_api_key\\n\"\n        \"VOYAGE_API_KEY=your_voyage_api_key\"\n    )\n\n# Load variables from .env\nwith open(env_path) as f:\n    for line in f:\n        if '=' in line and not line.strip().startswith('#'):\n            key, value = line.strip().split('=', 1)\n            os.environ[key] = value.strip('\"\\'')\n\nshow_info(\"Loaded environment variables from .env file\")\n\n# Check required environment variables\nrequired_vars = [\"MONGODB_URI\", \"GOOGLE_API_KEY\", \"VOYAGE_API_KEY\"]\nmissing_vars = [var for var in required_vars if not os.getenv(var)]\n\nif missing_vars:\n    raise ValueError(\n        f\"âŒ Missing required environment variables in .env file: {missing_vars}\\n\"\n        \"Please add these to your .env file:\\n\" + \n        \"\\n\".join([f\"{var}=your_{var.lower()}\" for var in missing_vars])\n    )\n\nshow_success(\"All required environment variables are set!\")\nshow_info(\"âœ“ MONGODB_URI: Available\")\nshow_info(\"âœ“ GOOGLE_API_KEY: Available\") \nshow_info(\"âœ“ VOYAGE_API_KEY: Available\")\n\n# Validate connection variables\ntry:\n    validator.validate_variable_exists(\"MONGODB_URI\", {\"MONGODB_URI\": os.getenv(\"MONGODB_URI\")}, str)\nexcept NameError:\n    pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB Atlas\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "SERVERLESS_URL = os.getenv(\"SERVERLESS_URL\")  # Optional fallback\n",
    "LLM_PROVIDER = \"google\"\n",
    "\n",
    "# Initialize MongoDB client\n",
    "try:\n",
    "    mongodb_client = MongoClient(MONGODB_URI)\n",
    "    # Test the connection\n",
    "    result = mongodb_client.admin.command(\"ping\")\n",
    "    \n",
    "    if result.get(\"ok\") == 1:\n",
    "        show_success(\"Successfully connected to MongoDB Atlas! ðŸŽ‰\")\n",
    "        \n",
    "        # Mark step as complete\n",
    "        try:\n",
    "            progress.mark_done(\"Environment Setup\", score=100, notes=\"MongoDB connection successful\")\n",
    "        except NameError:\n",
    "            pass\n",
    "    else:\n",
    "        show_error(\"MongoDB connection failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Connection error: {e}\")\n",
    "    show_hint(\"Check your connection string and network access settings\", \n",
    "             \"Connection Troubleshooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: VoyageAI Client Setup\n",
    "\n",
    "Initialize the VoyageAI Python client for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"VoyageAI Client Setup\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Setting up VoyageAI client...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import voyageai\nimport requests\nimport numpy as np\n\n# Initialize VoyageAI client with required API key\nVOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n\n# Use direct API key (required)\nvoyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)\nshow_success(\"VoyageAI client initialized with API key! ðŸš€\")\n\n# Normalize vector function (MongoDB doesn't auto-normalize)\ndef normalize_vector(v):\n    \"\"\"Normalize a vector to unit length.\"\"\"\n    norm = np.linalg.norm(v)\n    return v / norm if norm > 0 else v\n\nshow_success(\"Vector normalization utility ready\")\n\n# Mark step complete\ntry:\n    progress.mark_done(\"VoyageAI Client Setup\", score=100, \n                      notes=\"VoyageAI client configured with direct API key\")\nexcept NameError:\n    pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: PDF Processing\n",
    "\n",
    "Download a research paper and extract pages as images for multimodal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"PDF Processing\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Processing PDF and extracting images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory for images\n",
    "Path(\"data/images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "show_info(\"ðŸ“š Reference: https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the DeepSeek paper\n",
    "try:\n",
    "    show_info(\"Downloading DeepSeek R1 research paper...\")\n",
    "    response = requests.get(\"https://arxiv.org/pdf/2501.12948\")\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "    \n",
    "    # Get the content of the response\n",
    "    pdf_stream = response.content\n",
    "    show_success(f\"PDF downloaded successfully! Size: {len(pdf_stream)} bytes\")\n",
    "    \n",
    "    # Open the data in `pdf_stream` as a PDF document\n",
    "    pdf = pymupdf.Document(stream=pdf_stream, filetype=\"pdf\")\n",
    "    \n",
    "    show_success(f\"PDF loaded! Pages: {pdf.page_count}\")\n",
    "    \n",
    "    # Validate PDF processing\n",
    "    try:\n",
    "        validator.validate_variable_exists('pdf', locals(), pymupdf.Document)\n",
    "        validator.validate_custom(\n",
    "            pdf.page_count > 0,\n",
    "            \"PDF has valid page count\",\n",
    "            \"PDF appears to be empty or corrupted\"\n",
    "        )\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"PDF processing failed: {e}\")\n",
    "    show_hint(\"Check your internet connection and try again\", \"Download Issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pages as images\n",
    "from tqdm import tqdm\n",
    "\n",
    "docs = []\n",
    "zoom = 3.0\n",
    "\n",
    "show_info(\"ðŸ“š Reference: https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap\")\n",
    "\n",
    "try:\n",
    "    # Set image matrix dimensions\n",
    "    mat = pymupdf.Matrix(zoom, zoom)\n",
    "    \n",
    "    show_info(f\"Extracting {pdf.page_count} pages as images...\")\n",
    "    \n",
    "    # Track partial progress\n",
    "    total_pages = pdf.page_count\n",
    "    \n",
    "    # Iterate through the pages of the PDF\n",
    "    for n in tqdm(range(pdf.page_count), desc=\"Extracting pages\"):\n",
    "        temp = {}\n",
    "        \n",
    "        # Use the `get_pixmap` method to render the PDF page\n",
    "        pix = pdf[n].get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Store image locally\n",
    "        key = f\"data/images/{n+1}.png\"\n",
    "        pix.save(key)\n",
    "        \n",
    "        # Extract image metadata\n",
    "        temp[\"key\"] = key\n",
    "        temp[\"width\"] = pix.width\n",
    "        temp[\"height\"] = pix.height\n",
    "        temp[\"page_number\"] = n + 1\n",
    "        docs.append(temp)\n",
    "    \n",
    "    show_success(f\"Successfully extracted {len(docs)} pages as images!\")\n",
    "    show_info(f\"Images saved to: data/images/\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"PDF Processing\", score=95, \n",
    "                          notes=f\"Extracted {len(docs)} pages\")\n",
    "    except (NameError, AttributeError):\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Image extraction failed: {e}\")\n",
    "    show_hint(\"Ensure the data/images directory exists and is writable\", \"File Access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Embedding Generation with VoyageAI Client\n",
    "\n",
    "Generate multimodal embeddings using the VoyageAI Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Embedding Generation\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Generating embeddings with VoyageAI client...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from PIL import Image\nimport json\nimport base64\nfrom io import BytesIO\n\ndef generate_embedding(data, input_type=\"document\", model=\"voyage-multimodal-3\"):\n    \"\"\"Generate embedding using VoyageAI client.\n    \n    Args:\n        data: PIL Image or text string\n        input_type: \"document\" or \"query\" (affects embedding optimization)\n        model: Model to use for embedding generation\n    \n    Returns:\n        list: Normalized embedding vector\n    \"\"\"\n    try:\n        # Use VoyageAI Python client - it handles PIL Images directly\n        if isinstance(data, Image.Image):\n            # For images, use multimodal embedding with proper input format\n            inputs = [[data]]  # VoyageAI expects nested list format\n            response = voyage_client.multimodal_embed(\n                inputs=inputs, \n                model=model, \n                input_type=input_type\n            )\n            embedding = response.embeddings[0]\n        else:\n            # For text, use regular embedding\n            response = voyage_client.embed(\n                texts=[str(data)],\n                model=\"voyage-2\",  # Use text model for text\n                input_type=input_type\n            )\n            embedding = response.embeddings[0]\n        \n        # Log usage for cost tracking\n        if hasattr(response, 'usage'):\n            show_info(f\"Token usage: {response.usage}\")\n        \n        # Normalize the embedding (MongoDB doesn't do this automatically)\n        normalized_embedding = normalize_vector(np.array(embedding)).tolist()\n        \n        # Log vector norm for quality checking\n        norm = np.linalg.norm(normalized_embedding)\n        if abs(norm - 1.0) > 0.01:  # Should be close to 1.0 after normalization\n            show_warning(f\"Vector norm after normalization: {norm:.4f} (expected ~1.0)\")\n        \n        return normalized_embedding\n        \n    except Exception as e:\n        show_error(f\"Embedding generation failed: {e}\")\n        return None\n\nshow_success(\"Embedding generation function ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all extracted images\n",
    "embedded_docs = []\n",
    "\n",
    "try:\n",
    "    show_info(f\"Generating embeddings for {len(docs)} images...\")\n",
    "    \n",
    "    # Process images in batches for efficiency\n",
    "    batch_size = 10  # Adjust based on API limits\n",
    "    \n",
    "    for i in tqdm(range(0, len(docs), batch_size), desc=\"Processing batches\"):\n",
    "        batch = docs[i:i+batch_size]\n",
    "        \n",
    "        for doc in batch:\n",
    "            try:\n",
    "                # Load the image\n",
    "                img = Image.open(doc['key'])\n",
    "                \n",
    "                # Generate embedding using the new client\n",
    "                embedding = generate_embedding(img, input_type=\"document\")\n",
    "                \n",
    "                if embedding:\n",
    "                    doc[\"embedding\"] = embedding\n",
    "                    embedded_docs.append(doc)\n",
    "                    \n",
    "                    # Validate embedding properties\n",
    "                    if len(embedding) != 1024:\n",
    "                        show_warning(f\"Unexpected embedding dimension: {len(embedding)} (expected 1024)\")\n",
    "                else:\n",
    "                    show_warning(f\"Failed to generate embedding for {doc['key']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                show_error(f\"Error processing {doc['key']}: {e}\")\n",
    "    \n",
    "    show_success(f\"Successfully generated embeddings for {len(embedded_docs)} documents!\")\n",
    "    \n",
    "    # Save embeddings to file for future use\n",
    "    Path(\"data\").mkdir(exist_ok=True)\n",
    "    with open(\"data/embeddings_voyageai.json\", \"w\") as f:\n",
    "        json.dump(embedded_docs, f)\n",
    "    \n",
    "    show_info(\"Embeddings saved to data/embeddings_voyageai.json\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"Embedding Generation\", score=100, \n",
    "                          notes=f\"Generated {len(embedded_docs)} embeddings\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Batch embedding generation failed: {e}\")\n",
    "    show_hint(\"Check your API key and rate limits\", \"API Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Data Ingestion\n",
    "\n",
    "Ingest the generated embeddings into MongoDB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DB_NAME = \"mongodb_aiewf\"\n",
    "COLLECTION_NAME = \"multimodal_workshop_voyageai\"\n",
    "\n",
    "# Connect to the collection\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "show_info(f\"Connected to database: {DB_NAME}\")\n",
    "show_info(f\"Using collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data into MongoDB\n",
    "show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many\")\n",
    "\n",
    "try:\n",
    "    # Clear existing documents\n",
    "    delete_result = collection.delete_many({})\n",
    "    show_info(f\"Deleted {delete_result.deleted_count} existing documents\")\n",
    "    \n",
    "    # Use the newly generated embeddings or load from file\n",
    "    data_to_ingest = embedded_docs if embedded_docs else []\n",
    "    \n",
    "    if not data_to_ingest:\n",
    "        # Try to load from saved file\n",
    "        try:\n",
    "            with open(\"data/embeddings_voyageai.json\", \"r\") as f:\n",
    "                data_to_ingest = json.load(f)\n",
    "            show_info(f\"Loaded {len(data_to_ingest)} documents from saved embeddings\")\n",
    "        except FileNotFoundError:\n",
    "            show_error(\"No embeddings data available for ingestion\")\n",
    "            raise\n",
    "    \n",
    "    # Bulk insert documents into the collection\n",
    "    insert_result = collection.insert_many(data_to_ingest)\n",
    "    \n",
    "    # Verify insertion\n",
    "    doc_count = collection.count_documents({})\n",
    "    \n",
    "    show_success(f\"Successfully ingested {doc_count} documents into {COLLECTION_NAME}! ðŸŽ‰\")\n",
    "    \n",
    "    # Validate ingestion\n",
    "    try:\n",
    "        validator.validate_custom(\n",
    "            doc_count == len(data_to_ingest),\n",
    "            \"All documents ingested successfully\",\n",
    "            f\"Document count mismatch: expected {len(data_to_ingest)}, got {doc_count}\"\n",
    "        )\n",
    "        \n",
    "        progress.mark_done(\"Data Ingestion\", score=100, \n",
    "                          notes=f\"Ingested {doc_count} documents\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Data ingestion failed: {e}\")\n",
    "    show_hint(\"Check your MongoDB connection and permissions\", \"Database Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Vector Search Index Creation\n",
    "\n",
    "Create a vector search index to enable similarity search on our multimodal embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show step guidance\n",
    "try:\n",
    "    progress.show_step_tips(\"Vector Index Creation\")\n",
    "except (NameError, AttributeError):\n",
    "    show_info(\"Creating vector search index...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VS_INDEX_NAME = \"vector_index_voyageai\"\n",
    "\n",
    "# Define vector index configuration\n",
    "model = {\n",
    "    \"name\": VS_INDEX_NAME,\n",
    "    \"type\": \"vectorSearch\",\n",
    "    \"definition\": {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"numDimensions\": 1024,\n",
    "                \"similarity\": \"cosine\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "show_info(f\"Index configuration: {VS_INDEX_NAME}\")\n",
    "show_info(\"Vector field: embedding\")\n",
    "show_info(\"Dimensions: 1024 (Voyage multimodal)\")\n",
    "show_info(\"Similarity metric: cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector search index\n",
    "show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index\")\n",
    "\n",
    "try:\n",
    "    # Check if index already exists\n",
    "    existing_indexes = list(collection.list_search_indexes())\n",
    "    index_exists = any(idx.get('name') == VS_INDEX_NAME for idx in existing_indexes)\n",
    "    \n",
    "    if index_exists:\n",
    "        show_info(f\"Index '{VS_INDEX_NAME}' already exists\")\n",
    "    else:\n",
    "        show_info(\"Creating vector search index...\")\n",
    "        \n",
    "        # Create the vector search index\n",
    "        collection.create_search_index(model=model)\n",
    "        \n",
    "        show_success(f\"Vector search index '{VS_INDEX_NAME}' created successfully! ðŸŽ‰\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"Vector Index Creation\", score=100, \n",
    "                          notes=f\"Index '{VS_INDEX_NAME}' ready\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Index creation failed: {e}\")\n",
    "    show_hint(\"Index creation may take a few minutes. Check Atlas UI to monitor progress\", \n",
    "             \"Index Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index status\n",
    "try:\n",
    "    indexes = list(collection.list_search_indexes())\n",
    "    \n",
    "    show_info(\"Current search indexes:\")\n",
    "    for idx in indexes:\n",
    "        name = idx.get('name', 'Unknown')\n",
    "        status = idx.get('status', 'Unknown')\n",
    "        \n",
    "        if status == 'READY':\n",
    "            show_success(f\"âœ… {name}: {status}\")\n",
    "        else:\n",
    "            show_warning(f\"â³ {name}: {status}\")\n",
    "    \n",
    "    # Check if our index is ready\n",
    "    our_index = next((idx for idx in indexes if idx.get('name') == VS_INDEX_NAME), None)\n",
    "    \n",
    "    if our_index and our_index.get('status') == 'READY':\n",
    "        show_success(f\"Index '{VS_INDEX_NAME}' is ready for vector search! ðŸš€\")\n",
    "    else:\n",
    "        show_warning(f\"Index '{VS_INDEX_NAME}' is still building. Please wait...\")\n",
    "        show_hint(\"Index creation can take several minutes. Check the Atlas UI for progress.\", \n",
    "                 \"Index Building\")\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Failed to check index status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Agent Tools Setup\n",
    "\n",
    "Create the vector search tool using the VoyageAI client for query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import List\n\nshow_info(\"ðŸ“š Reference: https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_information_for_question_answering(user_query: str) -> List[str]:\n    \"\"\"\n    Retrieve information using vector search to answer a user query.\n    Uses VoyageAI client for query embedding generation.\n\n    Args:\n        user_query (str): The user's query string.\n\n    Returns:\n        List[str]: List of image file paths retrieved from vector search.\n    \"\"\"\n    try:\n        show_info(f\"ðŸ” Searching for: {user_query}\")\n        \n        # Generate query embedding using VoyageAI client\n        query_embedding = generate_embedding(user_query, input_type=\"query\")\n        \n        if not query_embedding:\n            show_error(\"Failed to generate query embedding\")\n            return []\n        \n        show_success(f\"Generated query embedding: {len(query_embedding)} dimensions\")\n\n        # Define aggregation pipeline with $vectorSearch and $project stages\n        pipeline = [\n            {\n                \"$vectorSearch\": {\n                    \"index\": VS_INDEX_NAME,\n                    \"path\": \"embedding\",\n                    \"queryVector\": query_embedding,\n                    \"numCandidates\": 150,  # Higher for better recall\n                    \"limit\": 2,  # Top results\n                }\n            },\n            {\n                \"$project\": {\n                    \"_id\": 0,\n                    \"key\": 1,\n                    \"width\": 1,\n                    \"height\": 1,\n                    \"page_number\": 1,\n                    \"score\": {\"$meta\": \"vectorSearchScore\"},\n                }\n            },\n        ]\n\n        # Execute the aggregation pipeline\n        results = list(collection.aggregate(pipeline))\n        \n        # Extract image keys and scores\n        keys = [result[\"key\"] for result in results]\n        scores = [result[\"score\"] for result in results]\n        \n        show_success(f\"Found {len(keys)} relevant images\")\n        for i, (key, score) in enumerate(zip(keys, scores)):\n            show_info(f\"  {i+1}. {key} (score: {score:.4f})\")\n        \n        return keys\n        \n    except Exception as e:\n        show_error(f\"Vector search failed: {e}\")\n        return []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function declaration for Gemini function calling\n",
    "show_info(\"ðŸ“š Reference: https://ai.google.dev/gemini-api/docs/function-calling#step_1_define_function_declaration\")\n",
    "\n",
    "# Define the function declaration\n",
    "get_information_for_question_answering_declaration = {\n",
    "    \"name\": \"get_information_for_question_answering\",\n",
    "    \"description\": \"Retrieve information using vector search to answer a user query. Uses VoyageAI embeddings for enhanced similarity matching.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"user_query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Query string to use for vector search\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"user_query\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "show_success(\"Function declaration created for Gemini integration!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Agent Tools Setup\", score=100, \n",
    "                      notes=\"Vector search tool with VoyageAI client ready\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: LLM Integration\n",
    "\n",
    "Set up Gemini 2.0 Flash with function calling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google import genai\nfrom google.genai import types\nfrom google.genai.types import FunctionCall\n\nLLM = \"gemini-2.0-flash\"\n\ntry:\n    # Use GOOGLE_API_KEY from environment (required)\n    api_key = os.getenv(\"GOOGLE_API_KEY\")\n    \n    # Initialize Gemini client\n    gemini_client = genai.Client(api_key=api_key)\n    \n    show_success(f\"Gemini client initialized with model: {LLM}\")\n    show_info(\"Using GOOGLE_API_KEY from environment\")\n    \n    # Validate client setup\n    try:\n        validator.validate_variable_exists('gemini_client', locals(), genai.Client)\n    except NameError:\n        pass\n        \nexcept Exception as e:\n    show_error(f\"LLM setup failed: {e}\")\n    show_hint(\"Check your GOOGLE_API_KEY in .env file\", \"API Key Error\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generation configuration\n",
    "try:\n",
    "    tools = types.Tool(\n",
    "        function_declarations=[get_information_for_question_answering_declaration]\n",
    "    )\n",
    "    tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)\n",
    "    \n",
    "    show_success(\"Generation configuration created with function calling enabled!\")\n",
    "    show_info(\"Temperature: 0.0 (deterministic responses)\")\n",
    "    show_info(\"Available tools: get_information_for_question_answering\")\n",
    "    \n",
    "    # Mark step complete\n",
    "    try:\n",
    "        progress.mark_done(\"LLM Integration\", score=100, \n",
    "                          notes=\"Gemini 2.0 Flash configured with function calling\")\n",
    "    except NameError:\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    show_error(f\"Configuration failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Basic Agent Implementation\n",
    "\n",
    "Create the core agent functions for tool selection and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info(\"ðŸ“š Reference: https://ai.google.dev/gemini-api/docs/function-calling#step_4_create_user_friendly_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tool(messages: List) -> FunctionCall | None:\n",
    "    \"\"\"\n",
    "    Use an LLM to decide which tool to call.\n",
    "\n",
    "    Args:\n",
    "        messages (List): Messages as a list\n",
    "\n",
    "    Returns:\n",
    "        FunctionCall: Function call object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = [\n",
    "            (\n",
    "                \"You're an AI assistant. Based on the given information, decide which tool to use. \"\n",
    "                \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image. \"\n",
    "                \"Here is the provided information:\\n\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Input to the LLM\n",
    "        contents = system_prompt + messages\n",
    "        \n",
    "        # Generate response using Gemini\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=LLM, contents=contents, config=tools_config\n",
    "        )\n",
    "        \n",
    "        # Extract and return the function call\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].function_call\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Tool selection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "show_success(\"Tool selection function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_answer(user_query: str, images: List = []) -> str:\n    \"\"\"\n    Execute any tools and generate a response.\n\n    Args:\n        user_query (str): User's query string\n        images (List): List of image file paths. Defaults to [].\n\n    Returns:\n        str: LLM-generated response\n    \"\"\"\n    try:\n        show_info(\"ðŸ” DEBUG: Starting generate_answer\")\n        show_info(f\"ðŸ” DEBUG: User query: {user_query}\")\n        show_info(f\"ðŸ” DEBUG: Initial images: {images}\")\n        \n        # Use select_tool to determine if we need to call any tools\n        tool_call = select_tool([user_query])\n        \n        show_info(f\"ðŸ” DEBUG: Tool call result: {tool_call}\")\n        \n        # If a tool call is found and it's our vector search function\n        if (\n            tool_call is not None\n            and tool_call.name == \"get_information_for_question_answering\"\n        ):\n            show_info(f\"ðŸ› ï¸ Agent calling tool: {tool_call.name}\")\n            show_info(f\"ðŸ” DEBUG: Tool call args: {tool_call.args}\")\n            \n            # Call the tool with the extracted arguments\n            tool_images = get_information_for_question_answering(**tool_call.args)\n            \n            show_info(f\"ðŸ” DEBUG: Tool returned {len(tool_images) if tool_images else 0} images\")\n            if tool_images:\n                show_info(f\"ðŸ” DEBUG: Image paths: {tool_images}\")\n            \n            # Add retrieved images to the input images\n            images.extend(tool_images)\n        else:\n            show_warning(\"ðŸ” DEBUG: No tool was called!\")\n\n        show_info(f\"ðŸ” DEBUG: Total images to send to LLM: {len(images)}\")\n        show_info(f\"ðŸ” DEBUG: Image paths: {images}\")\n\n        # Prepare system prompt\n        system_prompt = (\n            \"Answer the questions based on the provided context only. \"\n            \"If the context is not sufficient, say I DON'T KNOW. \"\n            \"DO NOT use any other information to answer the question.\"\n        )\n        \n        # Verify images exist and can be opened\n        valid_images = []\n        for img_path in images:\n            try:\n                img = Image.open(img_path)\n                valid_images.append(img)\n                show_success(f\"âœ… DEBUG: Successfully opened image: {img_path}\")\n            except Exception as e:\n                show_error(f\"âŒ DEBUG: Failed to open image {img_path}: {e}\")\n        \n        show_info(f\"ðŸ” DEBUG: Successfully opened {len(valid_images)} images\")\n        \n        # Prepare contents for the LLM\n        contents = [system_prompt] + [user_query] + valid_images\n\n        show_info(f\"ðŸ” DEBUG: Sending to LLM - prompt + query + {len(valid_images)} images\")\n\n        # Get the response from the LLM\n        response = gemini_client.models.generate_content(\n            model=LLM,\n            contents=contents,\n            config=types.GenerateContentConfig(temperature=0.0),\n        )\n        \n        answer = response.text\n        show_info(f\"ðŸ” DEBUG: LLM response length: {len(answer)} characters\")\n        return answer\n        \n    except Exception as e:\n        show_error(f\"Answer generation failed: {e}\")\n        import traceback\n        show_error(f\"ðŸ” DEBUG: Full traceback:\\n{traceback.format_exc()}\")\n        return \"I apologize, but I encountered an error while processing your question.\"\n\nshow_success(\"Answer generation function with debugging created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_agent(user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the agent and display the response.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ¤– Processing query: {user_query}\")\n",
    "        \n",
    "        response = generate_answer(user_query, images)\n",
    "        \n",
    "        show_success(\"ðŸ¤– Agent Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"Agent execution function created!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Basic Agent Testing\", score=100, \n",
    "                      notes=\"Agent functions with VoyageAI integration ready\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with different types of queries\n",
    "show_info(\"ðŸ§ª Testing the agent with sample queries...\")\n",
    "\n",
    "# Test 1: Text-based query requiring vector search\n",
    "# Expected: The Pass@1 accuracy of DeepSeek R1 on AIME 2024 is 79.8%.\n",
    "\n",
    "show_info(\"Test 1: Factual question requiring document search\")\n",
    "execute_agent(\"What is the Pass@1 accuracy of DeepSeek R1 on AIME 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# DEBUG: Check MongoDB and index status\nshow_info(\"ðŸ” DEBUG: Checking MongoDB connection and index\")\n\n# Check collection\ntry:\n    doc_count = collection.count_documents({})\n    show_success(f\"âœ… Collection has {doc_count} documents\")\n    \n    # Get a sample document to check structure\n    sample_doc = collection.find_one()\n    if sample_doc:\n        show_info(f\"Sample document keys: {list(sample_doc.keys())}\")\n        if 'embedding' in sample_doc:\n            show_success(f\"âœ… Embedding field exists, length: {len(sample_doc['embedding'])}\")\n        else:\n            show_error(\"âŒ No embedding field in documents!\")\n            \n        if 'key' in sample_doc:\n            show_info(f\"Sample image path: {sample_doc['key']}\")\n    else:\n        show_error(\"âŒ No documents found in collection\")\n        \nexcept Exception as e:\n    show_error(f\"âŒ MongoDB error: {e}\")\n\n# Check index status\ntry:\n    indexes = list(collection.list_search_indexes())\n    show_info(f\"Search indexes: {indexes}\")\n    \n    for idx in indexes:\n        if idx.get('name') == VS_INDEX_NAME:\n            status = idx.get('status', 'Unknown')\n            if status == 'READY':\n                show_success(f\"âœ… Index {VS_INDEX_NAME} is READY\")\n            else:\n                show_error(f\"âŒ Index {VS_INDEX_NAME} status: {status}\")\nexcept Exception as e:\n    show_error(f\"âŒ Index check error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# DEBUG: Test vector search directly\nshow_info(\"ðŸ” DEBUG: Testing vector search function directly\")\ntest_query = \"What is the Pass@1 accuracy of DeepSeek R1 on AIME 2024?\"\nshow_info(f\"Test query: {test_query}\")\n\nresults = get_information_for_question_answering(test_query)\nshow_info(f\"ðŸ” DEBUG: Vector search returned: {results}\")\n\nif results:\n    # Try to verify the images exist\n    for img_path in results:\n        import os\n        if os.path.exists(img_path):\n            show_success(f\"âœ… Image exists: {img_path}\")\n            # Try to open it\n            try:\n                test_img = Image.open(img_path)\n                show_success(f\"âœ… Can open image: {img_path} - Size: {test_img.size}\")\n            except Exception as e:\n                show_error(f\"âŒ Cannot open image: {e}\")\n        else:\n            show_error(f\"âŒ Image does NOT exist: {img_path}\")\nelse:\n    show_warning(\"âš ï¸ No results returned from vector search\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Image explanation\n",
    "import os\n",
    "\n",
    "if docs and len(docs) > 0:\n",
    "    show_info(\"Test 2: Document page analysis\")\n",
    "    execute_agent(\"What can you see in this document page?\", [docs[0]['key']])\n",
    "else:\n",
    "    show_warning(\"No document pages available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Memory Implementation\n",
    "\n",
    "Add conversational memory to enable multi-turn conversations with context retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Set up history collection\n",
    "history_collection = mongodb_client[DB_NAME][\"history_voyageai\"]\n",
    "\n",
    "show_info(f\"Setting up conversation memory in: {DB_NAME}.history_voyageai\")\n",
    "show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for efficient session queries\n",
    "try:\n",
    "    # Create index on session_id field\n",
    "    history_collection.create_index(\"session_id\")\n",
    "    \n",
    "    show_success(\"Session index created for conversation history!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    show_error(f\"Index creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Create chat history document and store it in MongoDB.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID\n",
    "        role (str): Message role, one of 'user' or 'agent'\n",
    "        type (str): Type of message, one of 'text' or 'image'\n",
    "        content (str): Content of the message (text or image path)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create message document\n",
    "        message = {\n",
    "            \"session_id\": session_id,\n",
    "            \"role\": role,\n",
    "            \"type\": type,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now(),\n",
    "        }\n",
    "        \n",
    "        # Insert message into history collection\n",
    "        history_collection.insert_one(message)\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Failed to store chat message: {e}\")\n",
    "\n",
    "show_success(\"Chat message storage function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID\n",
    "\n",
    "    Returns:\n",
    "        List: List of messages (text and images)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(\"ðŸ“š Reference: https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort\")\n",
    "        \n",
    "        # Query history collection and sort by timestamp\n",
    "        cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "        \n",
    "        messages = []\n",
    "        if cursor:\n",
    "            for msg in cursor:\n",
    "                # If message type is text, append content as is\n",
    "                if msg[\"type\"] == \"text\":\n",
    "                    messages.append(msg[\"content\"])\n",
    "                # If message type is image, open and append the image\n",
    "                elif msg[\"type\"] == \"image\":\n",
    "                    try:\n",
    "                        messages.append(Image.open(msg[\"content\"]))\n",
    "                    except Exception as e:\n",
    "                        show_warning(f\"Could not load image {msg['content']}: {e}\")\n",
    "        \n",
    "        return messages\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Failed to retrieve session history: {e}\")\n",
    "        return []\n",
    "\n",
    "show_success(\"Session history retrieval function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced generate_answer function with memory\ndef generate_answer_with_memory(session_id: str, user_query: str, images: List = []) -> str:\n    \"\"\"\n    Execute tools and generate response with conversation memory.\n\n    Args:\n        session_id (str): Session ID for conversation tracking\n        user_query (str): User's query string\n        images (List): List of image file paths. Defaults to [].\n\n    Returns:\n        str: LLM-generated response\n    \"\"\"\n    try:\n        # Retrieve conversation history\n        history = retrieve_session_history(session_id)\n        \n        show_info(f\"Retrieved {len(history)} previous messages for session {session_id}\")\n        \n        # Determine if tools need to be called\n        tool_call = select_tool(history + [user_query])\n        \n        if (\n            tool_call is not None\n            and tool_call.name == \"get_information_for_question_answering\"\n        ):\n            show_info(f\"ðŸ› ï¸ Agent calling tool: {tool_call.name}\")\n            tool_images = get_information_for_question_answering(**tool_call.args)\n            images.extend(tool_images)\n\n        # Generate response with history context\n        system_prompt = (\n            \"Answer the questions based on the provided context only. \"\n            \"If the context is not sufficient, say I DON'T KNOW. \"\n            \"DO NOT use any other information to answer the question.\"\n        )\n        \n        contents = (\n            [system_prompt]\n            + history\n            + [user_query]\n            + [Image.open(image) for image in images]\n        )\n        \n        response = gemini_client.models.generate_content(\n            model=LLM,\n            contents=contents,\n            config=types.GenerateContentConfig(temperature=0.0),\n        )\n        \n        answer = response.text\n        \n        # Store conversation in memory\n        # Store user query\n        store_chat_message(session_id, \"user\", \"text\", user_query)\n        \n        # Store image references\n        for image in images:\n            store_chat_message(session_id, \"user\", \"image\", image)\n        \n        # Store agent response\n        store_chat_message(session_id, \"agent\", \"text\", answer)\n        \n        return answer\n        \n    except Exception as e:\n        show_error(f\"Memory-enabled answer generation failed: {e}\")\n        return \"I apologize, but I encountered an error while processing your question.\"\n\nshow_success(\"Memory-enabled answer generation function created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced execute_agent function with memory\n",
    "def execute_agent_with_memory(session_id: str, user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the agent with conversation memory.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID for conversation tracking\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ§  Session {session_id} - Processing: {user_query}\")\n",
    "        \n",
    "        response = generate_answer_with_memory(session_id, user_query, images)\n",
    "        \n",
    "        show_success(\"ðŸ¤– Agent Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"Memory-enabled agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"Memory-enabled agent execution function created!\")\n",
    "\n",
    "# Mark step complete\n",
    "try:\n",
    "    progress.mark_done(\"Memory Implementation\", score=100, \n",
    "                      notes=\"Conversation memory system implemented\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory-enabled agent\n",
    "show_info(\"ðŸ§ª Testing memory-enabled agent...\")\n",
    "\n",
    "# First query in session\n",
    "show_info(\"Test 1: Initial query\")\n",
    "execute_agent_with_memory(\n",
    "    \"session_voyageai_1\",\n",
    "    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up query to test memory\n",
    "show_info(\"Test 2: Follow-up query to test memory\")\n",
    "execute_agent_with_memory(\n",
    "    \"session_voyageai_1\",\n",
    "    \"What did I just ask you?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: ReAct Agent Enhancement\n",
    "\n",
    "Implement a ReAct (Reasoning + Acting) agent that can reason about whether it has enough information and iteratively gather more data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_answer_react(user_query: str, images: List = []) -> str:\n    \"\"\"\n    Implement a ReAct (Reasoning + Acting) agent with VoyageAI embeddings.\n\n    Args:\n        user_query (str): User's query string\n        images (List): List of image file paths. Defaults to [].\n\n    Returns:\n        str: LLM-generated response\n    \"\"\"\n    try:\n        show_info(\"ðŸ§  Starting ReAct agent processing with VoyageAI embeddings...\")\n        \n        # Define reasoning prompt\n        system_prompt = [\n            (\n                \"You are an AI assistant with access to high-quality VoyageAI embeddings for document search. \"\n                \"Based on the current information, decide if you have enough to answer the user query, or if you need more information. \"\n                \"If you have enough information, respond with 'ANSWER: <your answer>'. \"\n                \"If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise. \"\n                f\"User query: {user_query}\\n\"\n                \"Current information:\\n\"\n            )\n        ]\n        \n        # Set max iterations to prevent infinite loops\n        max_iterations = 3\n        current_iteration = 0\n        \n        # Initialize list to accumulate information\n        current_information = []\n\n        # If the user provided images, add them to current information\n        if len(images) != 0:\n            current_information.extend([Image.open(image) for image in images])\n            show_info(f\"Added {len(images)} user-provided images to context\")\n\n        # Run the reasoning â†’ action loop\n        while current_iteration < max_iterations:\n            current_iteration += 1\n            show_info(f\"ðŸ”„ ReAct Iteration {current_iteration}:\")\n            \n            # Generate reasoning and decision\n            response = gemini_client.models.generate_content(\n                model=LLM,\n                contents=system_prompt + current_information,\n                config=types.GenerateContentConfig(temperature=0.0),\n            )\n            \n            decision = response.text\n            show_info(f\"ðŸ’­ Agent decision: {decision[:100]}...\")\n            \n            # If the agent has the final answer, return it\n            if \"ANSWER:\" in decision:\n                final_answer = decision.split(\"ANSWER:\", 1)[1].strip()\n                show_success(f\"âœ… Final answer reached in {current_iteration} iterations\")\n                return final_answer\n            \n            # If the agent decides to use a tool\n            elif \"TOOL:\" in decision:\n                tool_query = decision.split(\"TOOL:\", 1)[1].strip()\n                show_info(f\"ðŸ› ï¸ Agent requesting tool with query: {tool_query}\")\n                \n                # Use tool selection to get the function call\n                tool_call = select_tool([tool_query])\n                \n                if (\n                    tool_call is not None\n                    and tool_call.name == \"get_information_for_question_answering\"\n                ):\n                    show_info(f\"ðŸ“Š Calling VoyageAI-powered vector search with: {tool_call.args}\")\n                    \n                    # Call the tool and add results to current information\n                    tool_images = get_information_for_question_answering(**tool_call.args)\n                    \n                    if tool_images:\n                        new_images = [Image.open(image) for image in tool_images]\n                        current_information.extend(new_images)\n                        show_success(f\"âž• Added {len(new_images)} retrieved images to context\")\n                    else:\n                        show_warning(\"No relevant images found\")\n                        current_information.append(\"No relevant visual information found for this query.\")\n                else:\n                    show_warning(\"Tool selection failed or returned unexpected tool\")\n                    current_information.append(\"Tool call failed.\")\n            else:\n                show_warning(\"Agent response didn't contain ANSWER or TOOL directive\")\n                current_information.append(\"Unable to determine next action.\")\n        \n        # If we've exhausted iterations without a final answer\n        show_warning(f\"âš ï¸ Reached maximum iterations ({max_iterations}) without final answer\")\n        return \"I apologize, but I couldn't find a definitive answer after exploring the available information. Please try rephrasing your question or asking for more specific details.\"\n        \n    except Exception as e:\n        show_error(f\"ReAct agent failed: {e}\")\n        return \"I apologize, but I encountered an error while processing your question with the ReAct approach.\"\n\nshow_success(\"ReAct agent with VoyageAI integration completed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_react_agent(user_query: str, images: List = []) -> None:\n",
    "    \"\"\"\n",
    "    Execute the ReAct agent.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): User query\n",
    "        images (List, optional): List of image file paths. Defaults to [].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        show_info(f\"ðŸ¦¸â€â™€ï¸ ReAct Agent Processing: {user_query}\")\n",
    "        \n",
    "        response = generate_answer_react(user_query, images)\n",
    "        \n",
    "        show_success(\"ðŸ¤– ReAct Agent Final Response:\")\n",
    "        print(f\"\\n{response}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        show_error(f\"ReAct agent execution failed: {e}\")\n",
    "\n",
    "show_success(\"ReAct agent execution function created!\")\n",
    "\n",
    "# Mark final step complete\n",
    "try:\n",
    "    progress.mark_done(\"ReAct Agent Enhancement\", score=100, \n",
    "                      notes=\"ReAct reasoning and acting agent with VoyageAI implemented\")\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ReAct agent\n",
    "show_info(\"ðŸ§ª Testing ReAct agent with VoyageAI embeddings...\")\n",
    "\n",
    "# Test 1: Question requiring document search\n",
    "show_info(\"Test 1: Complex factual question\")\n",
    "execute_react_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Document analysis\n",
    "if docs and len(docs) > 0:\n",
    "    show_info(\"Test 2: Document page analysis with ReAct\")\n",
    "    execute_react_agent(\"What technical concepts are discussed in this document page?\", [docs[0]['key']])\n",
    "else:\n",
    "    show_warning(\"No document pages available for ReAct testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Workshop Complete!\n",
    "\n",
    "Congratulations! You've successfully built a comprehensive multimodal AI agent system with VoyageAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final progress summary\n",
    "try:\n",
    "    show_success(\"ðŸŽ“ VoyageAI Workshop Completed Successfully!\")\n",
    "    \n",
    "    # Display final progress\n",
    "    progress.display_progress(detailed=True)\n",
    "    \n",
    "    # Show completion statistics\n",
    "    completion_rate = progress.get_completion_rate()\n",
    "    avg_score = progress.get_average_score()\n",
    "    \n",
    "    show_info(f\"ðŸ“Š Overall Completion: {completion_rate:.1f}%\")\n",
    "    if avg_score:\n",
    "        show_info(f\"ðŸ“ˆ Average Score: {avg_score:.1f}/100\")\n",
    "    \n",
    "    # Show what was accomplished\n",
    "    show_success(\"\"\"\n",
    "    ðŸš€ What You've Built with VoyageAI:\n",
    "    \n",
    "    âœ… PDF processing pipeline for multimodal content\n",
    "    âœ… VoyageAI Python client for high-quality embeddings\n",
    "    âœ… MongoDB Atlas vector search integration\n",
    "    âœ… AI agent with function calling capabilities\n",
    "    âœ… Conversational memory system\n",
    "    âœ… ReAct (Reasoning + Acting) agent architecture\n",
    "    âœ… Production-ready multimodal AI application\n",
    "    âœ… Optimized query vs document embedding types\n",
    "    âœ… Vector normalization and quality checks\n",
    "    \"\"\")\n",
    "    \n",
    "    # Next steps\n",
    "    show_info(\"\"\"\n",
    "    ðŸŽ¯ Next Steps with VoyageAI:\n",
    "    \n",
    "    â€¢ Experiment with different VoyageAI models (voyage-2 vs voyage-lite-02-instruct)\n",
    "    â€¢ Implement batch processing for large document collections\n",
    "    â€¢ Add VoyageAI reranking for improved search quality\n",
    "    â€¢ Monitor usage and costs with response.usage logging\n",
    "    â€¢ Integrate with production applications using proper API key management\n",
    "    â€¢ Explore hybrid search combining keywords and vector similarity\n",
    "    \"\"\")\n",
    "    \n",
    "except NameError:\n",
    "    show_success(\"ðŸŽ“ VoyageAI Workshop completed successfully!\")\n",
    "    show_info(\"All agent implementations with VoyageAI client are ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export progress analytics\n",
    "try:\n",
    "    if hasattr(progress, 'export_analytics_json'):\n",
    "        analytics_file = progress.export_analytics_json()\n",
    "        show_success(f\"ðŸ“„ Progress analytics exported to: {analytics_file}\")\n",
    "        \n",
    "        # Show summary\n",
    "        summary = progress.get_analytics_summary()\n",
    "        if summary:\n",
    "            show_info(f\"â±ï¸ Total session time: {summary.get('session_duration', 'N/A')} seconds\")\n",
    "            show_info(f\"ðŸ“ Total interactions: {summary.get('total_events', 'N/A')}\")\n",
    "except (NameError, AttributeError):\n",
    "    pass\n",
    "\n",
    "show_success(\"Thank you for completing the Multimodal Agents Workshop with VoyageAI! ðŸ™\")\n",
    "show_info(\"ðŸ”— Learn more about VoyageAI: https://www.voyageai.com/\")\n",
    "show_info(\"ðŸ“š VoyageAI Documentation: https://docs.voyageai.com/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}